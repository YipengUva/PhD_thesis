\chapter{PCA of binary genomics data} \label{chapter:2}
Genome wide measurements of genetic and epigenetic alterations are generating more and more high dimensional binary data. The special mathematical characteristics of binary data make the direct use of the classical PCA model to explore low dimensional structures less obvious. Although there are several PCA alternatives for binary data in the psychometric, data analysis and machine learning literature, they are not well known to the bioinformatics community. In this chapter we introduce the motivation and rationale of some parametric and nonparametric versions of PCA specifically geared for binary data. Using both realistic simulations of binary data as well as mutation, CNA and methylation data of the Genomic Determinants of Sensitivity in Cancer 1000 (GDSC1000) the methods are explored for their performance with respect to finding the correct number of components, overfit, finding back the correct low dimensional structure, variable importance etc. The results show that if a low dimensional structure exists in the data that most of the methods can find it. When assuming a probabilistic generating process is underlying the data, we recommend to use the parametric logistic PCA model (using the projection based approach), while when such an assumption is not valid and the data is considered as given, the nonparametric Gifi model is recommended.
\interfootnotelinepenalty=10000
\footnote{This chapter is based on Song, Y., Westerhuis, J.A., Aben, N., Michaut, M., Wessels, L.F. and Smilde, A.K., 2017. Principal component analysis of binary genomics data. Briefings in bioinformatics, 20(1), pp.317-329.}

\section{Background}
Binary measurements only have two possible outcomes, such as presence and absence, or true and false, which are usually labeled as ``1'' and ``0''. In many research problems, objects are characterized by multiple binary features, each depicting a different aspect of the object. In biological research, several examples of binary data sets can be found. Genome wide measurements of genetic and epigenetic alterations are generating more and more high dimensional binary data \cite{mclendon2008comprehensive, iorio2016landscape}. One example is the high throughput measurements of point mutation. Here, a feature is labeled as ``1'' when it is classified as mutated in a sample, ``0'' when it is not. Another often observed binary data set is the binarized version of copy number aberrations (CNA), which are gains and losses of segments in chromosomal regions. Segments are labeled as ``1'' when aberration is presents in a sample, otherwise ``0'' \cite{wu2014detecting}. DNA methylation data can also be discretized as binary features, where ``1'' indicates a high level of methylation and ``0'' means a low level \cite{iorio2016landscape}.

Compared to commonly used quantitative data, binary data has some special mathematical characteristics, which should be taken into account during the data analysis. In binary measurements, ``0'' and ``1'' are abstract representations of two exclusive categories rather than quantitative values 0 and 1. These two categories can also be encoded to any other two different labels, like ``-1'' and ``1'' or ``-'' and ``+'', without changing the meaning. Because ``1'' and ``0'' are only an abstract representation of two categories, they cannot be taken interpreted as quantitative data. Furthermore, the measurement error of binary data is discrete in nature. Binary measurement error occurs when the wrong label is assigned to an object, such as when a mutated gene is mis-classified as wild type. Therefore, the by default used Gaussian error assumption for continuous data in many statistical models is inappropriate for binary data analysis. Another aspect of binary data is that there can be an order in the two categories. For example, presence is often considered more important than absence. Finally, binary data can be generated from a discrete measurement process, but also a continuous measurement process \cite{young1980quantifying}.

PCA is one of the most popular methods in dimension reduction with numerous applications in biology, chemistry and many other disciplines \cite{jolliffe2002principal}. PCA can map data points, which are in a high dimensional space, to a low dimensional space with minimum loss of variation. The derived low dimensional features, which provide a parsimonious representation of the original high dimensional data, can be used in data visualization or for further statistical analysis.

Classical linear PCA methods are appropriate for quantitative data. The direct use of linear PCA on binary data does not take into account the distinct mathematical characteristics of binary data. In this chapter, we are going to introduce, compare and evaluate some of the PCA alternatives for binary data. First, the theory of the different approaches is introduced together with their model properties and how the different models are assessed. Then we will introduce three binary genomics data sets on which the models will be applied. Besides the real data, realistic simulations of binary data are used to uncover some of the properties of the different models.

\section{Theory}
There exist two separate directions in extending PCA for binary data; parametric and nonparametric. Parametric approaches are represented by logistic PCA methods, originating from the machine learning literature. In these methods, PCA is extended to binary data from a probabilistic perspective in a similar way as linear regression is extended to logistic linear regression \cite{collins2001generalization, schein2003generalized, landgraf2015generalized}. Nonparametric methods, originating from the psychometric and data analysis communities, include optimal scaling \cite{de2009gifi}, multiple correspondence analysis \cite{mori2016nonlinear} and many others \cite{kiers1989three}. In this direction, PCA is extended to binary data from a geometric perspective without probabilistic assumptions. The details for the motivation and rationale of above approaches for binary data will be explained later in this section. We will start by introducing classical PCA.

\subsection{Classical PCA}
Classical PCA can be expressed as a projection based approach (finding the low dimensional space that best represents a cloud of high dimensional points) following Pearson \cite{pearson1901lines}. The measurements of $J$ quantitative variables on $I$ objects result into a matrix $\mathbf{X}$($I \times J$) with $I$ rows and $J$ columns. The column vector form of the $i^{\text{th}}$ row of $\mathbf{X}$ is $\mathbf{x}_i \in \mathbb{R}^J$, which is taken as a point in $J$ dimensional space. Suppose that a low dimensional space is spanned by the columns of an orthogonal loading matrix $\mathbf{B}$($J \times R)$, $R \ll \text{min}(I,J)$. The orthogonal projection of $\mathbf{x}_i$ on this low dimensional space is $\mathbf{B}\mathbf{B}^{\text{T}}\mathbf{x}_i$. We find $\mathbf{B}$ by minimizing the Euclidean distance between the centered high dimensional points $\mathbf{x}_i, i=1\cdots I$, and their low dimensional projections:
\begin{equation}\label{chapter2_eq:1}
\begin{aligned}
& \underset{\bm{\mu},\mathbf{B}}{\text{min}}
& &\sum_{i}^{I}((\mathbf{x_i} - \bm{\mu}) - \mathbf{BB}^{\text{T}}(\mathbf{x_i} - \bm{\mu}))^{2}\\
& \text{subject to}
& &\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I},
\end{aligned}
\end{equation}
in which the column offset term $\bm{\mu}$($J \times 1$) is included to center the samples, and $\mathbf{I}$ is the identity matrix. The exact position of the centered $i^{\text{th}}$ data point $\mathbf{x}_i$ in this low dimensional space is represented by its $R$ dimensional score vector $\hat{\mathbf{a_i}}$, $\hat{\mathbf{a_{i}}} = \hat{\mathbf{B}}^{\text{T}}( \mathbf{x_i} - \hat{\bm{\mu}})$, where $\hat{\mathbf{B}}$ and $\hat{\bm{\mu}}$ are the estimated values of equation \ref{chapter2_eq:1}. In matrix form, we have $\hat{\mathbf{A}}=(\mathbf{X} - \mathbf{1}\hat{\bm{\mu}}^{\text{T}})\hat{\mathbf{B}}$, $\hat{\mathbf{a_i}}$ is the $i^{\text{th}}$ row of $\hat{\mathbf{A}}$; $\mathbf{1}$ is an $I$ dimensional vector of ones; the estimated offset $\hat{\bm{\mu}}$ contains the column means of $\mathbf{X}$ and $\mathbf{X} - \mathbf{1}\hat{\bm{\mu}}^{\text{T}}$ is the column centered $\mathbf{X}$.

Another approach to explain PCA is the reconstruction based approach \cite{zou2006sparse}. A high dimensional data point $\mathbf{x_i} \in \mathbb{R}^J$ is approximated by a linear function of the latent low dimensional score $\mathbf{a_i} \in \mathbb{R}^R$ with orthogonal coefficients $\mathbf{B}$, $\mathbf{x_i} \approx \bm{\mu} + \mathbf{Ba_i}$, $\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I}$, $\bm{\mu}$ is the offset term. Now, $\bm{\mu}$, $\mathbf{A}$ and $\mathbf{B}$ can be found simultaneously by minimizing the Euclidean distance between centered $\mathbf{x_i}, i\cdots I$, and their low dimensional linear approximations $\bm{\mu} + \mathbf{Ba_i}, i \cdots I$:
\begin{equation}\label{chapter2_eq:2}
\begin{aligned}
&\underset{\bm{\mu},\mathbf{A}, \mathbf{B}}{\text{min}}
&&\sum_{i}^{I}{\mathbf{( x_i-\bm{\mu} -Ba_i)^{2}}}\\
&\text{subject to}
&&\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I}.
\end{aligned}
\end{equation}

It is well known that the above two approaches for classical PCA are equivalent and the global optimal solution can be obtained from the $R$ truncated singular value decomposition (SVD) of the column centered $\mathbf{X}$ \cite{ten1993least}. The solution $\hat{\bm{\mu}}$ contains the column means of $\mathbf{X}$; $\hat{\mathbf{A}}$ is the product of the first $R$ left singular vectors and the diagonal matrix of first $R$ singular values; $\hat{\mathbf{B}}$ contains the first $R$ right singular vectors.

Above, the classical PCA was derived from a geometrical perspective. Bishop et al.\cite{tipping1999probabilistic} have derived another explanation for PCA from a probabilistic perspective, called probabilistic PCA. A high dimensional point $\mathbf{x_i}$ can be regarded as a noisy observation of the true data point $\bm{\theta_i} \in \mathbb{R}^J$ , which lies in a low dimensional space. The model can be expressed as $\mathbf{x_i} = \bm{\theta_i} + \bm{\epsilon_i}$ and $\bm{\theta_i} = \bm{\mu} + \mathbf{Ba_i}$, $\bm{\mu}$ is the offset term as before; $\mathbf{B}$ contains the coefficients; $\mathbf{a_{i}}$ represents the low dimensional score vector. The noise term $\bm{\epsilon_{i}}$ is assumed to follow a multivariate normal distribution with mean $\mathbf{0}$ and constant variance $\sigma^2$, $\bm{\epsilon_i} \sim N(0,\sigma^2\mathbf{I})$. Thus the conditional distribution of $\mathbf{x_{i}}$ is a normal distribution with mean $\bm{\theta_i}$ and constant variance, $\mathbf{x_i}|\bm{\mu},\mathbf{A},\mathbf{B} \sim N(\bm{\mu} + \mathbf{Ba_i},\sigma^2\mathbf{I})$. $\bm{\mu}$, $\mathbf{A}$ and $\mathbf{B}$ can be obtained by maximum likelihood estimation.
\begin{equation}\label{chapter2_eq:3}
\begin{aligned}
&\underset{\bm{\mu},\mathbf{A},\mathbf{B}}{\text{max}}
&& \sum_i^I{\log(p(\mathbf{x_i}|\bm{\mu},\mathbf{a_i},\mathbf{B}))}\\
&
&&=\sum_i^I{\log(N(\mathbf{x_i}|\bm{\mu} + \mathbf{Ba_i},\sigma^2\mathbf{I}))}\\
&\text{subject to}
&&\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I}.
\end{aligned}
\end{equation}

The above maximum likelihood estimation is equivalent to the least squares minimization in classical PCA from the perspective of frequentist statistics \cite{collins2001generalization}. One important implication is that all the elements in the observed matrix $\mathbf{X}$ are conditionally independent of each other given the offset $\bm{\mu}$, the score matrix $\mathbf{A}$ and the loading matrix $\mathbf{B}$, which is the key point for the further extension to binary data.

\subsection{Logistic PCA}
The probabilistic interpretation of PCA under multivariate normal distribution for the observed data provides a framework for the further generalization to other data types \cite{tipping1999probabilistic}. As the Gaussian assumption is only appropriate for continuous quantitative data, it is necessary to replace the Gaussian assumption by the Bernoulli distribution for binary observations in a similar way as from linear regression to logistic linear regression \cite{collins2001generalization, landgraf2015generalized, de2006principal}. The $ij^{\text{th}}$ element in observed matrix $\mathbf{X}$, $x_{ij}$, is a realization of the Bernoulli distribution with parameter $p_{ij}$, which is the $ij^{\text{th}}$ element in the probability matrix $\mathbf{\Pi}$. Specifically, the probability that $x_{ij}$ equals ``1" is $p_{ij}$. Similar to probabilistic PCA, all the elements in the observed matrix $\mathbf{X}$ are conditionally independent of each other given the parameter matrix $\mathbf{\Pi}$($I\times J$). The log likelihood for observation $\mathbf{X}$ given the probability matrix $\mathbf{\Pi}$ is as follows:
\begin{equation}\label{chapter2_eq:4}
l(\mathbf{\Pi}) = \sum_{i}^{I}\sum_{j}^{J}x_{ij}\log(p_{ij})+(1-x_{ij})\log(1-p_{ij}).
\end{equation}
The log-odds of $p_{ij}$ is $\theta_{ij}$, where $\theta_{ij} = \log(\frac{p_{ij}}{1-p_{ij}})$, which is the natural parameter of the Bernoulli distribution expressed in the exponential family form. Thus $p_{ij} = \phi(\theta_{ij}) = (1+ e^{-\theta_{ij}})^{-1}$ and $\phi()$ is called the logistic function. The log likelihood for observation $\mathbf{X}$ given log-odds $\mathbf{ \Theta }$ is represented as:
\begin{equation}\label{chapter2_eq:5}
l(\mathbf{\Theta}) = \sum_{i}^{I}\sum_{j}^{J}x_{ij}\log(\phi(\theta_{ij}))+(1-x_{ij})\log(1-\phi(\theta_{ij})).
\end{equation}
A low dimensional structure can be assumed to exist in the log-odds $\mathbf{\Theta}$($I\times J$) as $\mathbf{\Theta} = \mathbf{A}\mathbf{B}^{\text{T}} + \mathbf{1}\bm{\mu}^{\text{T}}$. Here $\mathbf{A}$ is the object score matrix for the log-odds $\mathbf{ \Theta}$; $\mathbf{B}$ is the loading matrix; $\bm{\mu}$ is the offset.

There are mainly two approaches to fit the model (equation \ref{chapter2_eq:5}), logistic PCA \cite{de2006principal} and projection based logistic PCA (logistic PPCA) \cite{landgraf2015generalized}. The main difference between these two approaches is whether $\mathbf{A}$ and $\mathbf{B}$ are estimated  simultaneously or sequentially. In the logistic PCA model, the score matrix $\mathbf{A}$ and loading matrix $\mathbf{B}$ are estimated simultaneously by alternating minimization \cite{collins2001generalization, udell2016generalized} or by a majorization-minimization (MM) algorithm \cite{de2006principal}.

On the other hand, logistic PPCA only estimates $\mathbf{B}$ directly. After $\mathbf{B}$ is estimated, $\mathbf{A}$ is obtained by a projection based approach in the same manner as classical PCA in equation \ref{chapter2_eq:1} \cite{landgraf2015generalized}. Score matrix $\mathbf{A}$ is the low dimensional representation of the log-odds $\mathbf{\widetilde{\Theta}}$ of the saturated model in the subspace spanned by $\mathbf{B}$. Details of the log-odds $\mathbf{\widetilde{\Theta}}$ from the saturated model will be shown latter. In matrix form, $\mathbf{A} = (\mathbf{\widetilde{\Theta }}-\mathbf{1}\bm{\mu}^{\text{T}})\mathbf{B}$, $\bm{\mu}$ is the offset term. Then the log-odds $\mathbf{\Theta}$ in equation \ref{chapter2_eq:5} can be represented as $\mathbf{\Theta} = (\mathbf{\widetilde{\Theta }}-\mathbf{1}\bm{\mu}^{\text{T}})\mathbf{B}\mathbf{B}^{\text{T}} + \mathbf{1}\bm{\mu}^{\text{T}}$. The estimation of parameters $\hat{\bm{\mu}}$ and $\hat{\mathbf{B}}$, can be obtained by maximizing the conditional log likelihood $l(\mathbf{\Theta})$ in equation \ref{chapter2_eq:5}. Then, the solution for the score matrix is $\hat{\mathbf{A}} = (\mathbf{\widetilde{\Theta }}-\mathbf{1}\hat{\bm{\mu}}^{\text{T}})\hat{\mathbf{B}}$.

Compared to logistic PCA, logistic PPCA has fewer parameters to estimate, and thus is less prone to overfitting. In addition, the estimation of the scores of new samples in logistic PCA involves an optimization problem while for logistic PPCA, it is a simple projection of the new data on $\hat{\mathbf{B}}$.

In a saturated model, there is a separate parameter for every individual observation. The model is over-parameterized and has perfect fit to the observed data. For quantitative data, the parameters of the saturated model are simple the observed data. For example, the parameters of the saturated PCA model on observed data matrix $\mathbf{X}$ are the matrix $\mathbf{X}$ itself. For binary data, the parameter (probability of success) of a saturated model for the observation ``1" is 1; for the observation ``0" is 0. Thus, the $ij^{\text{th}}$ element in $\mathbf{\widetilde{\Theta}}$ from the saturated logistic PCA model is $\widetilde{\theta}_{ij} = \log(\frac{x_{ij}}{1-x_{ij}})$. It is negative infinity when $x_{ij} = 0$; positive infinity when $x_{ij} = 1$. In order to project $\mathbf{\widetilde{\Theta}}$ onto the low dimensional space spanned by $\mathbf{B}$, one needs a finite $\mathbf{\widetilde{\Theta}}$. In logistic PPCA, positive and negative infinities in $\mathbf{\widetilde{\Theta}}$ are approximated by large numbers $m$ and $-m$. When $m$ is too large, the elements in the estimated probability matrix $\hat{\mathbf{\Pi}}$ for generating the binary observation $\mathbf{X}$ are close to 1 or 0; when $m$ is close to 0, the elements in $\hat{\mathbf{\Pi}}$ are close to 0.5. In the original paper of logistic PPCA, CV is used to select $m$ \cite{landgraf2015generalized}. In this paper we select $m = 2.94$ which corresponds to a probability of success 0.95. This can be interpreted as using probabilities 0.95 and 0.05 to approximate the probabilities 1 and 0 in the saturated model.

\subsection{Theory of nonlinear PCA with optimal scaling}
Another generalization of PCA to binary data is nonlinear PCA with optimal scaling (the Gifi method). This method was primarily developed for categorical data, of which binary data is a special case \cite{gifi_B_90, de2009gifi}. The basic idea is to quantify the binary variables to quantitative values by minimizing some loss functions. The quantified variables are then used in a linear PCA model. The $j^{\text{th}}$ column of $\mathbf{X}$, $\mathbf{ X_{*j}}$, is encoded into an $ I \times 2 $ indicator matrix $\mathbf{G_j}$. $\mathbf{G_j}$ has two columns, ``1'' and ``0''. If the $i^{\text{th}}$ object belongs to column ``1'', the corresponding element of $\mathbf{ G_j }$  is 1, otherwise it is 0. $\mathbf{A}$ is the $I \times R$ object score matrix, which is the representation of $\mathbf{X}$ in a low dimensional Euclidean space; $\mathbf{ Q_j }$ is a $2 \times R$ quantification matrix, which quantifies this $j^{\text{th}}$ binary variable to a quantitative value. For binary data, the rank of the quantification matrix $\mathbf{ Q_j }$ is constrained to 1.~This is the PCA solution in the Gifi method.~$\mathbf{ Q_j }$ can be expressed as $\mathbf{ Q_j} = \mathbf{z_j}\mathbf{w_j}^{\text{T}}$, where $\mathbf{z_j}$ is a two dimensional column vector with binary quantifications and $\mathbf{w_j}$ is the vector of weights for $R$ principal components. The loss function is expressed as:
\begin{equation}
\underset{\mathbf{A,z_j,w_j}}{\text{min}} \sum_{j=1}^{J} {\mathbf{ ( A-G_j z_j w_j )}^2},
\end{equation}
in which the score matrix $\mathbf{A}$ is forced to be centered and orthogonal, $\mathbf{1}^{\text{T}}\mathbf{A}=0$, $\mathbf{A}^{\text{T}}\mathbf{A}=\mathbf{I}$, to avoid trivial solutions. The loss function is optimized by alternating least squares algorithms. For binary data, nonlinear PCA with optimal scaling is equivalent to multiple correspondence analysis and to PCA on standardized variables \cite{kiers1989three}.

\section{Model properties}
\subsection{Offset}
Including the column offset term $\bm{\mu}$ in component models also implies that the column mean of score matrix is $\mathbf{0}$, i.e. $\mathbf{1}^{\text{T}}\mathbf{A}=\mathbf{0}$. Otherwise, the model is unidentifiable. In PCA and the Gifi method, the estimated $\hat{\bm{\mu}}$ equals the column mean of $\mathbf{X}$. Therefore, including $\bm{\mu}$ in the model has the same effect as column centering of $\mathbf{X}$. In logistic PPCA and logistic PCA, the $j^{\text{th}}$ element of $\bm{\mu}$, $\mu_{j}$, can be interpreted as the log-odds of the marginal probability of the $j^{\text{th}}$ variable. When only the offset $\bm{\mu}$ is included in the model, $\mathbf{\Theta} = \mathbf{1}\bm{\mu}^{\text{T}}$, the $j^{\text{th}}$ element of the solution $\hat{\bm{\mu}}$, $\hat{\mu_{j}}$, is the log-odds of the empirical marginal probability of the $j^{\text{th}}$ variable (the proportion of ``1" in the $j^{\text{th}}$ column). When more components are included, $\mathbf{\Theta} = \mathbf{1}\bm{\mu}^{\text{T}} + \mathbf{A}\mathbf{B}^{\text{T}}$, the solution $\hat{\bm{\mu}}$ is not unique. If an identical offset is required for comparing component models with a different number of components, one can fix the offset term to the log-odds of the empirical marginal probability during the maximum likelihood estimation.

\subsection{Orthogonality}
Similar to PCA, the orthogonality constraint $\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I}$ in logistic PPCA and logistic PCA actually is inactive. If $\mathbf{B}$ is not orthogonal, it can be made orthogonal by subjecting $\mathbf{A}\mathbf{B}^{\text{T}}$ to an SVD algorithm. $\mathbf{B}$ equals the right hand singular vectors and $\mathbf{A}$ equals the product of the left hand singular vectors and the diagonal matrix of singular values. This extra step will not change the objective value. Table \ref{chapter2_table:01} gives the orthogonality properties of the scores and loadings of the four methods discussed above.
\begin{table}[htbp]
\centering
\caption{Orthogonality properties of the scores and loadings of the four methods. O: the columns of this matrix are orthonormal vectors, $\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{I}$; D: the columns of this matrix are orthogonal vectors, $\mathbf{B}^{\text{T}}\mathbf{B} = \mathbf{D}$, $\mathbf{D}$ is a $R \times R$ diagonal matrix.}
\label{chapter2_table:01}
\begin{tabular}{|c|c|c|c|c|}
  \hline
                                & PCA & Gifi & logistic PCA & logistic PPCA \\
    score matrix $\mathbf{A}$   & D & O & D & D \\
    loading matrix $\mathbf{B}$ & O & D & O & O \\
    \hline
\end{tabular}
\end{table}

\subsection{Nestedness}
Linear PCA models are nested in the number of components, which means the first $R$ principal components in the $R+1$ components model are exactly the same as the $R$ components model. For the Gifi method, this property only holds for the binary data case but not in general. For logistic PPCA and logistic PCA, this property does not hold.

\section{Model assessment}

\subsection{Error metric}
To make a fair comparison between linear PCA, the Gifi method, logistic PPCA and logistic PCA, the training error is defined as the average misclassification rate in using the derived low dimensional structure to fit the training set $\mathbf{X}$. Each of the four methods provides an estimation of the offset term, score matrix and loading matrix, $\bm{\hat{\mu}}$, $\mathbf{\hat{A}}$ and $\mathbf{\hat{B}}$. For linear PCA and the Gifi method, we take $\mathbf{1}\bm{\hat{\mu}}^{\text{T}} + \mathbf{\hat{A}}\mathbf{\hat{B}}^{\text{T}}$ as an approximation of the binary matrix $\mathbf{X}$; for logistic PCA and logistic PPCA, $\phi(\mathbf{1}\bm{\hat{\mu}}^{\text{T}} + \mathbf{\hat{A}}\mathbf{\hat{B}}^{\text{T}})$ is used as an approximation for the probability matrix $\mathbf{\Pi}$, of which the observed matrix $\mathbf{X}$ was generated. Since both approximations are continuous, we need to select a threshold to discretize them to binary fitting.

In the discretization process, two misclassification errors exist. ``0" can be misclassified as ``1", which we call $err0$ and ``1" can be misclassified as ``0", which we call $err1$. $N_{err0}$ is the number of $err0$ in this process, and $N_{err1}$ is the number of $err1$; $N_0$ is the number of ``0" in the observed binary matrix $\mathbf{X}$, and $N_1$ is the number of ``1" in $\mathbf{X}$. A commonly used total error rate is given by $(N_{err0} + N_{err1})/(N_0 + N_1)$, which gives equal weights to these two errors. However, this can lead to undesirable results for imbalanced binary data, i.e. when the proportions of ``1" and ``0" are extreme. Usually, imbalanced binary data sets are common in real applications, where sometimes the proportion of ``1" in the observed matrix $\mathbf{X}$ can be less than 5\%. In such a case, $err0$ is more likely to occur than $err1$, and hence it seems inappropriate to give them equal weights. In imbalanced cases, a balanced error rate $0.5\times(N_{err0}/N_0 + N_{err1}/N_1)$ is more appropriate \cite{wei2013role}. To decide whether the predicted quantitative value represents a ``0" or a ``1", a threshold value has to be selected. This threshold value can be selected by minimizing the balanced error rate in a training set after which it can be applied to a test set in order to prevent biased (too optimistic) results.

\subsection{Cross validation}
The training error is an overly optimistic estimator of the generalization error, which can be intuitively understood as the average misclassification rate in predicting an independent test set. Thus, we use cross validation (CV) to approximate the generalization error. In this chapter, we use the CV algorithm named EM-Wold \cite{wold1978cross,Bro2008}. In this approach, validation sets of elements of the matrix $\mathbf{X}$ are selected in a diagonal style rather than a row wise style. The left out part is considered as missing. In this way the prediction of the left out part is independent of the left out part itself. It is possible to use this approach as all the component models in this paper can handle missing data. A 7-fold CV procedure was used for all calculations in this paper. In each of these folds, a component model is developed taking the missing data into account. The model is then used to make a prediction of the missing elements. This is repeated until all elements of $\mathbf{X}$ have been predicted in this way. The threshold of converting the continuous predictions to binary predictions in CV was the same as the one used in computing the training error.

\section{Data Sets}
\subsection{Real data sets}
The data we used is from the Genomic Determinants of Sensitivity in Cancer 1000 (GDSC1000) \cite{iorio2016landscape}. To facilitate the interpretability of the results, only three cancer types are included in the data analysis: BRCA (breast invasive carcinoma, 48 human cell lines), LUAD (lung adenocarcinoma, 62 human cell lines) and SKCM (skin cutaneous melanoma, 50 human cell lines). Each cell line is a sample in the data analysis. For these samples, three different binary data sets are available: mutation, copy number aberration (CNA) and methylation data. For the mutation data, there are 198 mutation variables. Each variable is a likely cancer driver or suppressor gene. A gene is labeled as ``1" when it is classified as mutated in a sample and as ``0" when classified as wild type. The mutation data is very imbalanced (supplemental Fig.~S2.1 a): roughly $2\%$ of the data matrix is labeled as ``1". The CNA data has 410 observed CNA variables. Each variable is a copy number region in a chromosome. It is labeled as ``1" for a specific sample when it is identified as aberrated and it is labeled as ``0" otherwise. The CNA data set is also imbalanced (supplemental Fig.~S2.1 b): roughly $7\%$ of the data matrix is labeled as ``1". For the methylation data, there are 38 methylation variables. Each variable is a CpG island located in gene promotor region. In each variable, ``1" indicates a high level of methylation and ``0" indicates a low level. The methylation data set is relatively balanced compared to other data sets (supplemental Fig.~S2.1 c): roughly $27\%$ of the data matrix is labeled as ``1".

\subsection{Simulated binary data sets}
Binary data matrices with an underlying low dimensional structure can be simulated either from a latent variable model or as the noise corrupted version of a structured binary data set. In the first case the data generating process is considered to provide a quantitative data set while there is a binary read out. In the second case the data generating process is considered to provide a binary data set. We use both of these two approaches to study the properties of different binary PCA methods.

\subsubsection{Simulated binary data based on the logistic PCA model}
Data sets with different degrees of imbalance and with low dimensional structures were simulated according to the logistic PCA model. The offset term $\bm{\mu}$ is used to control the degree of imbalance and the log-odds $\mathbf{\Theta}$ is defined to have a low dimensional structure. The observed binary matrix $\mathbf{X}$ is generated from the corresponding Bernoulli distributions.

Each element in the $J \times R$ loading matrix $\mathbf{B}$ is sampled from the standard normal distribution. The Gram-Schmidt algorithm is used to force $\mathbf{B}^{\text{T}}\mathbf{B}=\mathbf{I}_R$. $R$ is set to 3. The simulated $I \times R$ score matrix $\mathbf{A}$ has three group structures in the samples. $I$ samples are divided into three groups of equal size. The three group means are set manually to force sufficient difference between the groups. The first two group means are set to $\mathbf{a_1^*} = [2,-1,3]^{\text{T}}$ and $\mathbf{a_2^*} = [-1,3,-2]^{\text{T}}$. The third group mean is $\mathbf{a_3^*} = [0,0,0]^{\text{T}} - \mathbf{a_1^*} - \mathbf{a_2^*}$. The scores in first group are sampled from the multivariate normal distribution $N(\mathbf{a_1^*}, \mathbf{I}_R)$, the scores in second group from $N(\mathbf{a_2^*},\mathbf{I}_R)$ and the scores in the third group from $N(\mathbf{a_3^*}, \mathbf{I}_R)$. In this way, scores between groups are sufficiently different and scores within the same group are similar.

When the elements in $\mathbf{A}\mathbf{B}^{\text{T}}$ are close to $0$, the corresponding probabilities are close to 0.5. In this case, the binary observations are almost a random guess. When their absolute values are large, the corresponding probabilities are close to 1 or 0, the binary observations are almost deterministic. The scale of $\mathbf{A}\mathbf{B}^{\text{T}}$ should be in a reasonable interval, not too large and not too small. A constant $C$ is multiplied to $\mathbf{A}\mathbf{B}^{\text{T}}$ to control the scale for generating proper probabilities. In addition, the offset term $\bm{\mu}$ is included to control the degree of imbalance in the simulated binary data set. After $\mathbf{\Theta} = C\mathbf{A}\mathbf{B}^{\text{T}} + \mathbf{1}\bm{\mu}^{\text{T}}$ is simulated as above, it is transformed to the probability matrix $\mathbf{\Pi}$ by the logistic function $\phi()$ and $x_{ij}$ in $\mathbf{X}$ is a realization of Bernoulli distribution with parameter $p_{ij}$, which is the $ij^{\text{th}}$ element of probability matrix $\mathbf{\Pi}$.

\subsubsection{Simulated binary data based on noise corruption of pre-structured binary data}
Another approach of simulating binary data is by the noise corruption of a pre-structured data set. Compared to the latent variable model, this approach provides an intuitive understanding of the low dimensional structure in the observed binary data. Pre-structured binary data set $\mathbf{X}_{\text{true}}$ has structured and unstructured parts. The goal of the current simulation is to find the variables that belong to the structured part, while the goal of the previous simulation is to see how well the whole data set can be approximated. The structured part is simulated as follows. $R$ different $I$ dimensional binary vectors are simulated first, each element is sampled from the Bernoulli distribution with probability $p$, which is the degree of imbalance in the binary data simulation. Each of these $R$ binary vectors is replicated 10 times to form the structured part. In the unstructured part of $\mathbf{X}_{\text{true}}$, all the elements are randomly sampled from Bernoulli distribution with probability $p$. The observed binary data $\mathbf{X}$ is a noise corrupted version of the pre-structured binary data set $\mathbf{X}_{\text{true}}$. If the noise level is set to 0.1, all the elements in the binary data $\mathbf{X}_{\text{true}}$ have a probability of 0.1 to be bit-flipped. The observed binary matrix $\mathbf{X}$, has $R$ groups of 10 highly correlated variables and the other variables are not correlated. The $R$ groups are taken as the low dimensional structure. The above simulation process is illustrated in the supplemental Fig.~S2.4.\\

\section{Results}
All the computations are done in R \cite{RProject}. The linear PCA model is fitted using the SVD method after centering the data \cite{stacklies2007pcamethods}. The Gifi method is fitted using the alternating least squares approach by Homals package \cite{de2009gifi}. The logistic PCA and logistic PPCA models are fitted using an MM algorithm with offset term \cite{de2006principal, landgraf2015generalized}. The default stopping criterion is used for all the approaches.

\subsection{Balanced simulation}
The goal of this simulation is to evaluate the abilities of the four approaches in finding back the embedded low dimensional structures in the sample space and variable space. The simulation process is based on the logistic PCA model. The offset term $\bm{\mu}$ is set to $\mathbf{0}$ to simulate balanced binary data. The parameters are set to $I = 99, J = 50, R = 3, C = 10$. The simulated balanced binary data are shown in supplemental Fig.~S2.2. First a classical PCA on the simulated probability matrix $\mathbf{\Pi}$ and the log-odds $\mathbf{\Theta}$ was performed. Fig.~\ref{chapter2_fig:1} shows the score plots of these two PCA analyses. The difference between the score plots of linear PCA on $\mathbf{\Pi}$ (Fig.~\ref{chapter2_fig:1} a) and on log-odds $\mathbf{\Theta}$ (Fig.~\ref{chapter2_fig:1} b) is obvious. The scores of the linear PCA model on $\mathbf{\Pi}$ lie in the margin of the figure, while for $\mathbf{\Theta}$, they lie more in the center of the figure. This difference is related to the nonlinear logistic function $\phi()$, which transforms $\mathbf{\Theta}$ to $\mathbf{\Pi}$. Furthermore, PCA on the log-odds matrix describes more variation in the first two PCs.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_1.pdf}
    \caption{Score plot of the first two principal components (PCs) derived from linear PCA on the probability matrix $\mathbf{\Pi}$ (left) and  log-odds matrix $\mathbf{\Theta}$ (right) used in the binary data simulation. G1, G2 and G3 are three simulated groups in the samples.}
    \label{chapter2_fig:1}
\end{figure}

Logistic PCA, logistic PCA, Gifi and linear PCA are used to model the binary matrix $\mathbf{X}$. Two principal components are used. Offset terms are included in the model. The score plots produced by these different approaches are shown in Fig.~\ref{chapter2_fig:2}. The similarity between Fig.~\ref{chapter2_fig:1} and Fig.~\ref{chapter2_fig:2} indicates that the logistic PCA model approximates the underlying log-odds $\mathbf{\Theta}$ from the binary observation $\mathbf{X}$, while the other approaches approximate the probability matrix $\mathbf{\Pi}$.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_2.pdf}
    \caption{Score plot of first two PCs produced by the four different approaches. \textbf{a}: logistic PPCA; \textbf{b}: logistic PCA; \textbf{c}: the Gifi method; \textbf{d}: PCA. G1, G2 and G3 are three simulated groups in the samples.}
    \label{chapter2_fig:2}
\end{figure}

Another observation is that the score plots derived from logistic PPCA (Fig.~\ref{chapter2_fig:2} a), Gifi (Fig.~\ref{chapter2_fig:2} c) and linear PCA (Fig.~\ref{chapter2_fig:2} d) are very similar except for some scale differences. The similarity between the Gifi and linear PCA for balanced binary data set is understandable. For binary data, the Gifi method is equivalent to PCA on standardized binary variables. Since the proportion of ``1'' and ``0'' of each binary variable are similar in a balanced simulated data set, the column mean and standard deviation of each binary variable are close to 0.5. Thus the standardization of each binary variable will change 0 and 1 binary data to -1 and 1 data. Therefore, except for the difference in scale, Gifi and linear PCA are almost the same for balanced binary data. For logistic PPCA, the score matrix $\mathbf{A}$ is a low dimensional representation of the log-odds $\mathbf{ \widetilde{\Theta } }$ from the saturated model, $\mathbf{A} = (\mathbf{\widetilde{\Theta }}-\mathbf{1}\bm{\mu}^{\text{T}})\mathbf{B}$, and the $\mathbf{ \widetilde{\Theta } }$ is estimated by $2m(\mathbf{X}-1)$. This is equivalent to changing 0 and 1 to $-m$ and $m$. Thus, the true difference between linear PCA and logistic PPCA is how to find the low dimension spanned by loading matrix $\mathbf{B}$. Logistic PPCA finds it by minimizing the logistic loss and linear PCA finds it by minimizing the least squares loss.

The training error and CV error for different models are shown in Fig.~\ref{chapter2_fig:3}. We add the zero component model in which only the offset term $\bm{\mu}$ is included, as the baseline for evaluating the different methods with different numbers of components. The estimated offset $\hat{\bm{\mu}}$ in the zero component model is the column mean of $\mathbf{X}$ for PCA and the Gifi method, while it is the logit transform of the column mean of $\mathbf{X}$ for logistic PPCA and logistic PCA. All approaches successfully find the three components truely underlying the data. It can also be observed that logistic PCA is more eager to overfit the data. It shows a lower balanced error rate, but a higher CV error rate for more than three components compared to the other methods.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_3.pdf}
    \caption{The balanced training error (\textbf{a}) and CV error (\textbf{b}) for the balanced simulated data set produced by four different approaches with different number of components. \textbf{a}: training error; \textbf{b}: CV error. log ppca: logistic PPCA; log pca: logistic PCA; gifi: the Gifi method; pca: linear PCA.}
    \label{chapter2_fig:3}
\end{figure}

\subsection{Imbalanced simulation}
The goal of the imbalanced simulation is to evaluate the effect of imbalanced simulated data on the ability of the four approaches in finding back the underlying low dimensional structures in variable space. Since the offset $\bm{\mu}$ in logistic PCA model can be interpreted as the log-odds of marginal probabilities, we can use the log-odds of the empirical marginal probabilities from the real data sets with different degrees of imbalance as the offset in the simulation. The simulation process is based on the logistic PCA model. The offset term $\bm{\mu}$ is set to log-odds of column means of real data to simulate imbalanced binary data. The parameters $I, J$ are set to the size of corresponding real data. The constant $C$ is selected as 20, $R$ is set to 3. The simulated data is shown in supplemental Fig.~S2.3. We evaluate the effect of imbalanced binary data set on the different models' abilities of finding back the simulated low dimensional structure. The CV error plots of different models are shown in Fig.~\ref{chapter2_fig:4}. All the approaches are successful in finding back three significant PCs.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_9.pdf}
    \caption{CV error plot for simulated imbalanced data sets with different degrees of imbalance. \textbf{a}: similar as mutation data; \textbf{b}: similar as CNA data; \textbf{c}: similar as methylation data.}
    \label{chapter2_fig:4}
\end{figure}

\subsection{Feature selection}
For the assessment of feature importance, the binary data is simulated by noise corruption of a pre-structured binary data set. $I$ is 198; $J$ is 100; $R$ is set to 3. The degree of imbalance is set to 0.2, and the noise level is 0.1. The simulated data is shown in supplemental Fig.~S2.4. There are noisy corrupted structures in the first 30 variables. For feature selection purposes we estimate the importance of each feature in the model. This is performed as follows, $\frac{1}{3}(b_{j1}^2+b_{j2}^2+ b_{j3}^2)$, where $b_{j2}$ is the loading in the $2^{\text{nd}}$ PCs for $j^{\text{th}}$ variable, is taken as the importance measure. The process is repeated 15 times, the mean and standard deviation of the average squared loading for the 100 variables are shown in Fig.~\ref{chapter2_fig:5}. It can be observed that highly correlated binary variables have large loadings. The variance of the loadings derived from logistic PCA is much higher than other approaches. This indicates that the logistic PCA model cannot make stable estimation of loadings.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_10.pdf}
    \caption{Barplot with one standard deviation error bar of the mean square loadings of linear PCA model (\textbf{a}), the Gifi method (\textbf{b}), logistic PPCA (\textbf{c}) and logistic PCA (\textbf{d}).}
    \label{chapter2_fig:5}
\end{figure}

\subsection{Real data}
The binary mutation, CNA and methylation data sets are analysed using the four different approaches. The score plots and error plots from different approaches on the real mutation data set are shown in Fig.~\ref{chapter2_fig:6}. The CV results of PCA, Gifi and logistic PCA in Fig.~\ref{chapter2_fig:6} f do not support the assumption that a low dimensional structure exists in the mutation data. For the CV result of logistic PPCA (Fig.~\ref{chapter2_fig:6} f), the minimum CV error was achieved using three components. However, this minimum was only slightly lower than the zero component model. Although the CV result of logistic PPCA is ambiguous, we can observe four clusters in the score plot.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_4.pdf}
    \caption{Score plot of the first two PCs, training and CV error plot of the four different approaches for the mutation data. \textbf{a}: score plot of logistic PPCA ; \textbf{b}: score plot of logistic PCA; \textbf{c}: score plot of the Gifi method; \textbf{d}: score plot of PCA; \textbf{e}: training error plot; \textbf{f}: CV error plot. BRCA: breast invasive carcinoma; LUAD: lung adenocarcinoma; SKCM: skin cutaneous melanoma. The legend of the training error and CV error plot is the same as Fig.~\ref{chapter2_fig:3}.}
    \label{chapter2_fig:6}
\end{figure}

To explore the clusters in more detail, Fig.~\ref{chapter2_fig:7} shows the loading plot and score plots with different mutation status of the logistic PPCA model. With the corresponding loading values (Fig.~\ref{chapter2_fig:7} a) we determined that these clusters were largely defined by TP53, BRAF and KRAS mutation status. Interestingly, these genes also have the highest mutational load, suggesting that variables with a higher aberration frequency contain more information. Cluster 1 (c1 in Fig.~\ref{chapter2_fig:7} b) is BRAF-mutant and TP53-mutant type; while cluster 2 (c2 in Fig.~\ref{chapter2_fig:7} b) is BRAF-mutant and TP53-wild type. Cluster 3 (c3 in Fig.~\ref{chapter2_fig:7} c) mostly consists of BRAF-wild and TP53-mutant cell lines, a configuration that often occurs in all three analyzed cancer types. Cluster 4 (c4 in Fig.~\ref{chapter2_fig:7} c) contains BRAF-wild and TP53-wild type cell lines, which again is a configuration that occurs across cancer types. Finally, we observed sub-clusters of LUAD cell lines towards the bottom of cluster 3 and 4, which consist of KRAS-mutant cell lines (Fig.~\ref{chapter2_fig:7} d). As BRAF and KRAS mutations both activate the MAPK pathway in a similar fashion, double mutants are redundant and hence rarely observed. Our results are in line with this mutual exclusivity pattern: with the exception of a single BRAF/KRAS double mutant, we find BRAF mutations only in cluster 1 and 2 and KRAS mutations only in cluster 3 and 4. One notable exception of the above characterization of the clusters is CAL-51 (labeled in Fig.~\ref{chapter2_fig:7} c). Given its TP53 wild-type status, CAL-51 would be expected in cluster 4, but it actually resides in the bottom-left of cluster 3. This shift left is likely due to mutations in both SMARCA4 and PIK3CA, which have the third and fourth most negative loading values on PC1.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_5.pdf}
    \caption{Loading plot (\textbf{a}) and score plots of the first two PCs derived from logistic PPCA model on mutation data. The score plots (\textbf{b, c, d}) are labeled according to the mutation patterns. \textbf{b}: BRAF mutation labeled score plot; \textbf{c}: TP53 mutation labeled score plot; \textbf{d}: KRAS mutation labeled score plot. Red square: mutated; black dot: wild type. c1, c2, c3 and c4 are the plausible four clusters in the samples on mutation data.}
    \label{chapter2_fig:7}
\end{figure}

The score plots and error plots from different approaches on CNA data are shown in Fig.~\ref{chapter2_fig:8}. There is some evidence from the CV results from all the models in Fig.~\ref{chapter2_fig:8} f for a five dimensional structure in the data. However, in the score plots of Fig.~\ref{chapter2_fig:8}, the samples with different cancer types are not well separated and there is no clear evidence of natural clusters. Therefore, we do not zoom in further on this data type.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_6.pdf}
    \caption{Score plot of the first two PCs, training and CV error plot of the four different approaches for the CNA data. \textbf{a}: score plot of logistic PPCA; \textbf{b}: score plot of logistic PCA; \textbf{c}: score plot of the Gifi method; \textbf{d}: score plot of PCA; \textbf{e}: training error plot; \textbf{f}: CV error plot. The legends for the score plot and training error plot are the same as Fig.~\ref{chapter2_fig:4}.}
    \label{chapter2_fig:8}
\end{figure}

The score plots and error plots from different approaches on methylation data are shown in Fig.~\ref{chapter2_fig:9}. The three cancer types are well separated in all the score plots. The similar and specific structure in the score plots of logistic PPCA, the Gifi method and linear PCA may be related to the unique structure of the methylation data (supplemental Fig.~S2.1 c). Different cancer types have very different methylation patterns, represented by unique sets of features. In addition, there is some evidence from the CV results from all the models in Fig.~\ref{chapter2_fig:9} f for a two dimensional structure in the data.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_7.pdf}
    \caption{Score plot of the first two PCs, training and CV error plot of the four different approaches for the methylation data. \textbf{a}: score plot of logistic PPCA ; \textbf{b}: score plot of logistic PCA; \textbf{c}: score plot of the Gifi method; \textbf{d}: score plot of PCA; \textbf{e}: training error plot; \textbf{f}: CV error plot. The legends for the score plot and training error plot are the same as Fig.~\ref{chapter2_fig:4}.}
    \label{chapter2_fig:9}
\end{figure}

We use the score plot derived from logistic PPCA model on methylation data as an example to interpret the result. The first two principal components from the logistic PPCA applied to the methylation data show three clusters, which perfectly represent the three cancer types (Fig.~\ref{chapter2_fig:9} a). The corresponding loading values also roughly fall into three cancer type specific clusters (Fig.~\ref{chapter2_fig:10}), as most variables are exclusively non-zero in a single cancer type. Notable exceptions are GSTM1 and ARL17A, which are non-zero in two cancer types and hence each reside between two clusters, and variables GSTT1 and DUSP22, which are non-zero in all three cancer types and hence reside towards the center of the plot.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{Chapter2_Fig_8.pdf}
    \caption{Loading plot of logistic PPCA model on methylation data. The gene names corresponding to the methylation variables, which are interpreted in the paper, are labeled on the plot.}
    \label{chapter2_fig:10}
\end{figure}

\section{Discussion}
In this chapter, four methods were discussed that all aim to explore binary data using low dimensional scores and loadings. It was shown that each of the methods has different goals and therefore produces slightly different results. Linear PCA (without the standardization processing step) treats the binary data as quantitative data and tries to use low rank score and loading matrices to fit the quantitative data. For binary data, the quantification process in the Gifi method is simply a standardization of the binary variables. After quantification, the Gifi method tries to use low rank score and loading matrices to fit the quantified binary variables. Both logistic PPCA and logistic PCA assume that the binary data follows a Bernoulli distribution, and try to find an optimal estimation of the log-odds matrix, which lies in the low dimensional space. Logistic PCA tries to estimate the low dimensional log-odds matrix directly; while logistic PPCA estimates this matrix by the projection of the log-odds matrix from the saturated model on a approximated low dimensional space.

For all the four approaches it is not the degree of imbalance which is the problem. As shown in our simulation results, the low dimensional structure of binary variables, which can be simulated from a latent variable model or by noise corruption of a pre-structured binary data set, is the key issue for the results of data analysis, rather than the degree of imbalance of the data set. When there is a low dimensional structure in our simulation process, all the approaches can successfully find the correct number of components with different degrees of imbalance.

In both the analysis of the simulated data and of the real data, the performance of the linear PCA method, in the criteria of training error and CV error, is similar to other specially designed algorithms for binary data. In addition, since the global optimum of the linear PCA model can always be achieved, the solution is very stable. However, the linear PCA model on binary data obviously contradicts the mathematical characteristics of binary data and the assumptions of the linear PCA model itself. In addition, the fitted values, elements in the product of score and loading matrix, can only be regarded as an approximation to quantitative 0 and 1, and are thus difficult to interpret.

The results of linear PCA and the Gifi method are very similar, especially when the degree of imbalance in each variable is approximately equal. Furthermore, there are signs of overfitting in the analysis of the CNA data by the Gifi model. However, compared to linear PCA, the interpretability of the Gifi method is better. The mathematical characteristics of binary data are taken into account from the geometrical perspective and the solutions can be interpreted as an approximation of the optimally quantified binary variables.

On the other hand, logistic PPCA and logistic PCA methods take into account the mathematical characteristics of binary data from the probabilistic perspective. Fitted values, elements in the product of the derived score and loading matrices, can be interpreted as the log-odds for generating the binary data, and the log-odds can again be transformed to probability. The problem for logistic PCA is that it is not robust with respect to the score and loading estimation, although it is able to select the correct number of components \cite{de2006principal}. Since both score matrix $\mathbf{A}$ and loading matrix $\mathbf{B}$ are free parameters to fit in the optimization, the estimation of $\mathbf{A}\mathbf{B}^{\text{T}}$ will not hesitate to move to infinity to minimize the loss function. This represents itself in such a way that logistic PCA is prone to overfit (as can be seen from the CV results) and the large variation in the loading estimation. The non-robustness problem is mitigated in the logistic PPCA model. Since only the loading matrix $\mathbf{B}$ is freely estimated in logistic PPCA to find the optimal model, while the score matrix $\mathbf{A}$ is fixed given the loadings, the logistic PPCA model is less prone to overfitting. Thus, the estimation of the loadings of binary variables is more stable compared to logistic PCA. Furthermore, since the fitted values are the linear projection of the log-odds matrix of the saturated model, its scale is constrained by the scale of the approximate log-odds matrix of the saturated model, which can be specified in advance.

When assuming a probabilistic generating process is underlying the binary data we recommend to use the parametric logistic PPCA model. When such an assumption is not valid and the data is considered as given, the nonparametric Gifi model is recommended.

\section*{Acknowledgements}
Y.S. gratefully acknowledges the financial support from China Scholarship Council (NO.201504910809). The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) / ERC synergy grant agreement $\text{n}^{0}$ 319661 COMBATCANCER.

\clearpage
\section{Supplementary information}
% chapter 2
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_S1.pdf}
    \label{chapter2_fig:S1}
    \caption*{Figure S2.1: Heatmap of the real data sets. White color: ``0''; black color: ``1''. \textbf{a}: mutation data; \textbf{b}: CNA data; \textbf{c}: methylation data. BRCA: breast invasive carcinoma; LUAD: lung adenocarcinoma; SKCM: skin cutaneous melanoma.}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{Chapter2_Fig_S2.pdf}
    \caption*{Figure S2.2: Heatmap of the simulated balanced data set with low dimensional structure. White color: ``0''; black color: ``1''. G1, G2 and G3 are three groups in the samples during simulation.}
    \label{chapter2_fig:S2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_S3.pdf}
    \caption*{Figure S2.3: Heatmap of simulated imbalanced binary data with different degrees of imbalance. White color: ``0''; black color: ``1''. \textbf{a}: similar as mutation data; \textbf{b}: similar as CNA data; \textbf{c}: similar as methylation data. No group structures are in the samples.}
    \label{chapter2_fig:S3}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{Chapter2_Fig_S4.pdf}
    \caption*{Figure S2.4: Heatmap of the pre-structured binary data (\textbf{a}) and noise corrupted binary data (\textbf{b}). White color: ``0''; black color: ``1''.}
    \label{chapter2_fig:S4}
\end{figure}




