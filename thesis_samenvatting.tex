\samenvatting
In biologisch onderzoek wordt het steeds gebruikelijker meerdere hoog-di\-men\-sionale metingen op verschillende platformen aan hetzelfde biologische systeem uit te voeren. Deze van verschillende bronnen afkomstige metingen bieden de mogelijkheid tot een beter begrip van het bestudeerde systeem, maar brengen ook nieuwe statistische uitdagingen met zich mee. Al deze uitdagingen houden verband met de heterogeniteit van de dataverzamelingen. De eerste vorm van heterogeniteit ligt in het type van de gegevens. Zo zijn er verschillende typen omics-data, metabolomics, proteomics en RNAseq data in genomics, die ieder een eigen perspectief op de eigenschappen van het biologische systeem bieden. De tweede vorm van heterogeniteit ligt in de meetschaal van de data. De data kunnen op verschillende schalen gemeten worden, zoals binair, ordinale schaal, intervalschaal en ratioschaal. In dit proefschrift worden verschillende datafusiemethoden ontwikkeld waarmee één of beide soorten dataheterogeniteit aangepakt kunnen worden.

In hoofdstuk \ref{chapter:2} wordt een aantal, zowel parametrische als niet-para\-me\-tri\-sche, uitbreidingen van principale componenten analyse (PCA) vergeleken die specifiek betrekking hebben op binaire data. In deze uitbreidingen van PCA wordt op verschillende manieren rekening gehouden met de speciale wiskundige karakteristieken van binaire gegevens. We onderzochten de prestaties van deze uitbreidingen van PCA met betrekking tot het vinden van het juiste aantal componenten, overfitting, het vinden van de juiste laag-dimensionale structuur, het belang van variabelen, enz. door gebruik van zowel realistische simulaties van binaire data als van mutatiedata, `copy number aberrations' (CNA) en methylatiedata van het GDSC1000 project. Onze resultaten laten zien dat als er een laag-dimensionale structuur in de data aanwezig is, de meeste methoden deze kunnen vinden. Wij adviseren om het parametrisch logistisch PCA-model (op projectie gebaseerde benadering) te gebruiken als verondersteld wordt dat een stochastisch proces aan de data ten grondslag ligt. Als dit niet het geval is en de data als vast gegeven kan worden beschouwd, raden we aan het niet-parametrische Gifi-model te gebruiken.

In hoofdstuk \ref{chapter:3} hebben we een robuust logistisch PCA-model ontwikkeld met behulp van een niet-convexe drempelwaardenfunctie voor de singuliere waarden. Het veelbelovende logistische PCA-model voor binaire data heeft een probleem met over-fitting vanwege de gebruikte randvoorwaarde van een  exacte lage rang. We stellen voor om het over-fitten te verminderen door het logistische PCA-model te fitten met gebruik van een niet-convexe drempelwaardenfunctie voor singuliere waarden. Een efficiënt majorisatie-minimalisatie (MM) algoritme is geïmplementeerd om het model te fitten en een op missende waarden gebaseerde kruisvalidatie (KV) procedure is geïntroduceerd voor modelselectie. Bovendien hebben we het logistische PCA-model uitgedrukt op basis van de latente variabelen interpretatie van gegeneraliseerde lineaire modellen (GLMs). Niet alleen maakt de aanname van een structuur met lage rang het model beter te begrijpen, maar biedt ook een manier om de signaal-ruis verhouding in de simulatie van multivariate binaire data te definiëren. Onze experimenten met realistische simulaties van ongebalanceerde binaire data met een lage signaal-ruis verhouding laten zien dat modelselectie gebaseerd op KV-fouten goed in staat is het voorgestelde model te selecteren. Dit geselecteerde model is uitstekend in staat de onderliggende lage-rang structuur terug te vinden en werkt beter dan modellen met een exacte lage-rang randvoorwaarde die een convexe spoornormboete gebruiken.

In hoofdstuk \ref{chapter:4} ontwikkelden we een gegeneraliseerd simultaan componenten analyse (GSCA) model voor de fusie van binaire en kwantitatieve dataverzamelingen. Simultane componenten analyse (SCA) is \'e\'en van de standaard hulpmiddelen om de onderliggende afhankelijkheidsstructuur te onderzoeken die aanwezig is in meerdere kwantitatieve data sets die aan hetzelfde object gemeten zijn. Echter, SCA is niet geschikt als een deel van de data binair is. Daarom stellen we een GSCA-model voor dat rekening houdt met de specifieke mathematische eigenschappen van binaire data en kwantitatieve metingen binnen het kader van grootste aannemelijkheid. Op dezelfde manier als voor het SCA-model veronderstellen we het bestaan van een laag-dimensionale deelruimte waarin de gedeelde informatie van de twee typen van metingen wordt gerepresenteerd. Het GSCA-model is evenwel geneigd tot overfitten wanneer een rang groter dan 1 wordt gebruikt. Hierdoor kunnen sommige parameters bijzonder groot geschat worden. Om een oplossing met lage rang zonder overfitting te vinden, stellen we voor een niet-convexe drempelwaardefunctie te gebruiken voor de selectie van singuliere waarden. We ontwikkelden een effici\"ent majorisatie algoritme om dit model te fitten voor verschillende concave boetefuncties. Realistische simulaties (lage signaal-ruis verhouding en sterk ongebalanceerde binaire data) werden gebruikt om te beoordelen hoe goed het model de onderliggende structuur kan reproduceren. Ook is een op missende waarden gebaseerde kruisvalidatie geimplementeerd om modellen te selecteren. De bruikbaarheid van het GSCA-model als exploratieve tool wordt gedemonstreerd aan de hand van kwantitatieve genexpressiedata en binaire CNA-metingen uit de GDSC1000 dataverzameling.

In hoofdstuk \ref{chapter:5} stellen we een exponentieel SCA-model met boeteoptie (P-ESCA) voor om meerdere dataverzamelingen met twee typen heterogeniteit samen te voegen. Meerdere metingen aan hetzelfde object maar uitgevoerd op verschillende platforms kunnen complementaire informatie over het bestudeerde systeem opleveren. De heterogeniteit van deze data biedt interessante nieuwe statistische uitdagingen als de dataverzamelingen gefuseerd worden. Ten eerste is de scheiding van informatie die gemeenschappelijk is voor alle (of enkele) van de dataverzamelingen van de informatie die specifiek is voor iedere dataverzameling afzonderlijk, lastig. Bovendien zijn deze dataverzamelingen vaak een mix van kwantitatieve en discrete (binair of categorisch) data typen, terwijl gebruikelijke datafusiemethoden vereisen dat alle dataverzamelingen kwantitatief zijn. Met het door ons voorgestelde exponenti\"ele simultane componenten analyse (ESCA) model kunnen we dergelijke gemengde dataverzamelingen wel analyseren. Om de globale, lokaal gemeenschappelijke en verzameling-specifieke informatie in de verschillende dataverzamelingen te ontwarren, hebben we op de componentenladingsmatrix, via een groep van concave boetefuncties bijna zonder systematische fout, een gestructureerd, bijna leeg patroon opgelegd. Om het voorgestelde model te fitten hebben we een algoritme gebaseerd op majorisatie-minimalisatie gemaakt. Dit algoritme gebruikt analytische oplossingen om de modelparameters na iedere iteratie te actualiseren; de doelfunctie wordt door het algoritme iedere iteratie monotoon verminderd. Voor de modelselectie gebruiken we een op missende waarden gebaseerde kruisvalidatie. De voordelen van de voorgestelde methode in vergelijking met andere methoden werden beoordeeld met uitgebreide simulaties en de analyse van echte data uit een chronische lymfatische leukemie (CCL) studie.

In hoofdstuk \ref{chapter:6} beschouwen we verschillende uitbreidingen van het ontwikkelde P-ESCA-model met nieuwe boetesystemen (per element, per groep en element-groep samenstelling) en een nieuwe benadering voor modelselectie. We kijken ook naar de mogelijkheden die het semi-parametrische XPCA-model en de niet-parametrische matrixmethode bieden om dataverzamelingen met heterogene meetschalen aan te pakken.
 