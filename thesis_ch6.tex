\chapter{Outlook} \label{chapter:6}
In the proposed penalized exponential family SCA (P-ESCA) model (Chapter \ref{chapter:5}), a group concave penalty is used to induce group-wise sparse pattern on the loading matrix to disentangle the global, local common and distinct components. The P-ESCA model and the associated MM algorithm can be further generalized to include other types of penalties to induce more interesting sparse patterns on the loading matrix, which may be useful for the data analyst. In addition, the current developed model selection procedure has difficulty in tuning multiple tuning parameters. It will be worthwhile to explore other types of model selection approaches to address this issue. In the current thesis, the parametric exponential family distribution is used to tackle the heterogeneous measurement scales. There also existed other possible non-parametric and semi-parametric approaches. It worths to generalize them for the data fusion of multiple data sets with the two types of heterogeneity. Furthermore, it is also interesting to generalize the P-ESCA model for prediction tasks or to tacking into account the experimental design underlining the used multiple data sets.

\section{Including other types of sparse patterns}
The developed P-ESCA model and the associated MM algorithm have a lot of potential for further generalization. The options for inducing element-wise sparsity on the loading matrix or the composition of both group-wise and element-wise sparsity or other types of penalties can be easily included. Some examples will be shown in the following subsections. These P-ESCA model extensions can be selected using the developed missing value based CV procedure (Chapter \ref{chapter:5}). A possible alternative model selection approach is the Bayesian optimization \cite{frazier2018tutorial} framework, which is appropriate for the tuning of multiple (usually less than 20) continuous tuning parameters. Since this framework has been successfully applied in the automatic tuning of various machine learning algorithms \cite{snoek2012practical}, it will be worthwhile to explore its usage for the P-ESCA models with multiple tuning parameters.

The following notations are the same as the P-ESCA model in Chapter \ref{chapter:5}. Suppose the $r^{\text{th}}$ column of the $l^{\text{th}}$ loading matrix $\mathbf{B}_l$ is $\mathbf{b}_{l,r}$. The group concave penalty on the $l^{\text{th}}$ loading matrix $\mathbf{B}_l$ is imposed on the $L_2$ norm of $\mathbf{b}_{l,r}$ as $\lambda_l \sum_{r} g(||\mathbf{b}_{l,r}||_2)$, in which $g()$ is a concave function. This group concave penalty (or concave $L_2$ norm penalty) will shrink in the group level ($L_2$ norm of $\mathbf{b}_{l,r}$) as a concave penalty, and in the element level (elements inside $\mathbf{b}_{l,r}$) as a ridge regression type penalty. Sometimes we may need other types of penalties, such as the element-wise sparsity on the elements of $\mathbf{B}_l$ or the composition of both element-wise and group-wise sparsity patterns on $\mathbf{B}_l$. All these options can be easily included into the P-ESCA model by a slightly modification of the developed MM algorithm.

\subsection{P-ESCA model with an element-wise concave penalty}
When a single data set is used, the P-ESCA model with an element-wise concave penalty is an approach for the sparse exponential family PCA model. When multiple data sets are used, the model is an approach for the sparse exponential family SCA model.

\subsubsection{Element-wise concave penalty}
Element-wise concave penalty can be imposed on the elements of $\mathbf{B}_l$ to induce the element-wise sparsity on $\mathbf{B}_l$. Suppose the $jr^{\text{th}}$ element of $\mathbf{B}_l$ is $b_{jr}^{l}$, and its absolute value is $\sigma_{ljr}$, $\sigma_{ljr} = |b_{jr}^{l}|$. The concave penalty on $\mathbf{B}_l$ can be expressed as $\lambda_l \sum_{j}^{J_l}\sum_{r}^{R} g(\sigma_{ljr})$, in which $g()$ is a concave function in Table \ref{chapter5_tab:1}. The optimization problem associated with this P-ESCA model with an element-wise concave penalty can be expressed as follows,
\begin{equation}\label{chapter6_eq:1}
\begin{aligned}
    \min_{ \left\{\bm{\mu}_l\right\}_{l}^{L}, \mathbf{A}, \left\{\mathbf{B}_l\right\}_{l}^{L}} \quad & \sum_{l=1}^{L} \Big[ -\log(p(\mathbf{X}_l|\mathbf{\Theta}_l, \alpha_l)) + \lambda_l \sum_{j}^{J_l}\sum_{r}^{R} g(\sigma_{ljr}) \Big] \\
    \text{subject to} \quad \mathbf{\Theta}_l &= \mathbf{1}\bm{\mu}_l^{\text{T}} + \mathbf{AB}_l^{\text{T}}, \quad l = 1,\ldots,L \\
     \mathbf{1}^{\text{T}}\mathbf{A} &= \mathbf{0}\\
	 \mathbf{A}^{\text{T}}\mathbf{A} &= \mathbf{I} \\
	 \sigma_{ljr} &= |b_{jr}^{l}|, l = 1...L; j=1,\dots,J_l; r = 1,\ldots, R.
\end{aligned}
\end{equation}

\subsubsection{Algorithm}
The algorithm developed in Section \ref{section:5.3} can be modified only with respect to the majorization step of the penalty function and the updating $\mathbf{B}_l$ step to fit the P-ESCA model with an element-wise penalty in equation \ref{chapter6_eq:1}. Similar to the equation \ref{chapter5_eq:5}, the element-wise penalty $g(\sigma_{ljr})$ is a concave function with respect to $\sigma_{ljr}$ and can be majorized as $g(\sigma_{ljr}) \leq \omega_{ljr}^k \sigma_{ljr} + c$, in which $\omega_{ljr}^k = \partial g(\sigma_{ljr}^k)$ and $\sigma_{ljr}^k$ is the absolute value of the $jr^{\text{th}}$ element of $\mathbf{B}_l^k$ (the $k^{\text{th}}$ approximation of $\mathbf{B}_l$ during the $k^{\text{th}}$ iteration). After majorizing the original problem in equation \ref{chapter6_eq:1}, updating the offset terms $\{\mu_l \}_1^L$ and score matrix $\mathbf{A}$ in exactly the same way as in Section \ref{section:5.3}, the optimization problem associated with the updating of $\mathbf{B}_l$ is
\begin{equation*}
\begin{aligned}
    \min_{\mathbf{B}_l} \quad & \frac{\rho_l}{2\alpha_l} ||\mathbf{AB}_l^{\text{T}} - \mathbf{JH}_{l}^{k}||_F^2  + \lambda_l \sum_{j}^{J_l}\sum_{r}^{R} \omega_{ljr}^k \sigma_{ljr} \\
	&= \frac{\rho_l}{2\alpha_l}||\mathbf{B}_l - (\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A}||_F^2 + \lambda_l \sum_{j}^{J_l}\sum_{r}^{R} \omega_{ljr}^k \sigma_{ljr}\\
    &= \sum_{j}^{J_l} \sum_{r}^R \Big[ \frac{\rho_l}{2\alpha_l}(b_{jr}^l - ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr})^2 + \lambda_l \omega_{ljr}^k \sigma_{ljr} \Big]\\
    \sigma_{ljr} &= |b_{jr}^{l}|, l = 1...L; j=1,\dots,J_l; r = 1,\ldots, R,
\end{aligned}
\end{equation*}
in which $((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr}$ indicates the $jr^{\text{th}}$ element of the matrix $(\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A}$. The above optimization problem is simple the proximal operator of the $L_1$ norm, and the analytical solution exists \cite{parikh2014proximal}. Take $\tilde{\lambda}_{ljr} = \lambda_l \omega_{ljr}^k \alpha_l/\rho_l$ and $v_{ljr} = ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr}$, the analytical solution of $b_{jr}^{l}$ is $b_{jr}^{l} = \text{sign}(v_{ljr}) \max(0, |v_{ljr}| -\tilde{\lambda}_{ljr})$. To update the parameter $\mathbf{B}_l$, we can simply apply this proximal operator to all the elements of $\mathbf{B}_l$. The other parts of the algorithm are the same as in Section \ref{section:5.3}.

\subsection{P-ESCA model with a concave L1 norm penalty}
\subsubsection{Concave L1 norm penalty}
Another way to induce group sparsity on $\mathbf{B}_l$ is through the concave $L_1$ norm penalty \cite{huang2012selective}. Suppose the $r^{\text{th}}$ column of the $l^{\text{th}}$ loading matrix $\mathbf{B}_l$ is $\mathbf{b}_{l,r}$, and its $L_1$ norm is $\sigma_{lr}$, $\sigma_{lr} = ||\mathbf{b}_{l,r}||_1 = \sum_{j}^{J_l} |b_{jr}^l|$. The concave $L_1$ norm penalty on $\mathbf{B}_l$ can be expressed as $\lambda_l J_l \sum_{r}^{R} g(\sigma_{lr})$, in which weight $J_l$ is used to accommodate the potential different number of variables in different data set, and $g()$ is a concave function in Table \ref{chapter5_tab:1}. This concave $L_1$ norm penalty will shrink in the group level ($L_1$ norm of $\mathbf{b}_{l,r}$) as a concave penalty, and in the element level (elements inside $\mathbf{b}_{l,r}$) as a lasso type penalty. The optimization problem associated with this P-ESCA model with a concave $L_1$ norm penalty can be expressed as follows,
\begin{equation}\label{chapter6_eq:2}
\begin{aligned}
    \min_{ \left\{\bm{\mu}_l\right\}_{l}^{L}, \mathbf{A}, \left\{\mathbf{B}_l\right\}_{l}^{L}} \quad & \sum_{l=1}^{L} \Big[ -\log(p(\mathbf{X}_l|\mathbf{\Theta}_l, \alpha_l)) + \lambda_l J_l \sum_{r}^{R} g(\sigma_{lr}) \Big] \\
    \text{subject to} \quad \mathbf{\Theta}_l &= \mathbf{1}\bm{\mu}_l^{\text{T}} + \mathbf{AB}_l^{\text{T}}, \quad l = 1,\ldots,L \\
     \mathbf{1}^{\text{T}}\mathbf{A} &= \mathbf{0}\\
	 \mathbf{A}^{\text{T}}\mathbf{A} &= \mathbf{I} \\
	 \sigma_{lr} &= ||\mathbf{b}_{l,r}||_1, l = 1...L; r = 1,\ldots, R.
\end{aligned}
\end{equation}

\subsubsection{Algorithm}
The algorithm developed in Section \ref{section:5.3} can be modified only with respect to the majorization step of the penalty function and the updating $\mathbf{B}_l$ step to fit the P-ESCA model with the concave $L_1$ norm penalty in equation \ref{chapter6_eq:2}. Similar to the equation \ref{chapter5_eq:5}, the penalty function $g(\sigma_{lr})$ is concave with respect to $\sigma_{lr}$ and can be majorized as $g(\sigma_{lr}) \leq \omega_{lr}^k \sigma_{lr} + c$, in which $\omega_{lr}^k = \partial g(\sigma_{lr}^k)$ and $\sigma_{lr}^k$ is the $L_1$ norm of the $r^{\text{th}}$ column of $\mathbf{B}_l^k$ (the $k^{\text{th}}$ approximation of $\mathbf{B}_l$ during the $k^{\text{th}}$ iteration). After majorizing the original problem in equation \ref{chapter6_eq:2}, updating the offset terms $\{\mu_l \}_1^L$ and score matrix $\mathbf{A}$ in exactly the same way as in Section \ref{section:5.3}, the optimization problem associated with the updating of $\mathbf{B}_l$ is
\begin{equation*}
\begin{aligned}
    \min_{\mathbf{B}_l} \quad & \frac{\rho_l}{2\alpha_l} ||\mathbf{AB}_l^{\text{T}} - \mathbf{JH}_{l}^{k}||_F^2  + \lambda_l J_l \sum_{r}^{R} \omega_{lr}^k \sigma_{lr} \\
	&= \frac{\rho_l}{2\alpha_l}||\mathbf{B}_l - (\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A}||_F^2 + \lambda_l J_l \sum_{r}^{R} \omega_{lr}^k (\sum_{j}^{J_l} |b_{jr}^l|)\\
    &= \sum_{j}^{J_l} \sum_{r}^R \Big[ \frac{\rho_l}{2\alpha_l}(b_{jr}^l - ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr})^2 + \lambda_l J_l \omega_{lr}^k |b_{jr}^l| \Big].
\end{aligned}
\end{equation*}
Take $\tilde{\lambda}_{ljr} = \lambda_l J_l \omega_{lr}^k \alpha_l/\rho_l$ and $v_{ljr} = ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr}$, the analytical solution of $b_{jr}^{l}$ is $b_{jr}^{l} = \text{sign}(v_{ljr}) \max(0, |v_{ljr}| -\tilde{\lambda}_{ljr})$. To update the parameter $\mathbf{B}_l$, we can simply apply this proximal operator to all the elements of $\mathbf{B}_l$. The other parts of the algorithm are the same as in Section \ref{section:5.3}.

\subsection{P-ESCA model with a composite concave penalty}
\subsubsection{Composite concave penalty}
There also exists a composite concave penalty to induce both group and element-wise sparsity \cite{huang2012selective}. The composite concave penalty on $\mathbf{B}_l$ can be expressed as $\lambda_l J_l \sum_{r}^{R} g_{out}(\sum_{j}^{J_l} g_{inner}(|b_{jr}^l|))$, in which weight $J_l$ is used to accommodate the potential different number of variables in different data set, $g_{out}()$ and $g_{inner}()$ are two concave functions for the group level and element level respectively. We will use the same concave function $g()$ from Table \ref{chapter5_tab:1} for both $g_{out}()$ and $g_{inner}()$. This composite concave penalty will shrink both in the group level ($\sum_{j}^{J_l} g_{inner}(|b_{jr}^l|)$) and in the element level (elements inside $\mathbf{b}_{l,r}$) as a concave penalty. The optimization problem associated with this P-ESCA model with a composite concave penalty can be expressed as follows,
\begin{equation}\label{chapter6_eq:3}
\begin{aligned}
    \min_{ \left\{\bm{\mu}_l\right\}_{l}^{L}, \mathbf{A}, \left\{\mathbf{B}_l\right\}_{l}^{L}} \quad & \sum_{l=1}^{L} \Big[ -\log(p(\mathbf{X}_l|\mathbf{\Theta}_l, \alpha_l)) + \lambda_l J_l \sum_{r}^{R} g_{out}(\sum_{j}^{J_l} g_{inner}(|b_{jr}^l|)) \Big] \\
    \text{subject to} \quad \mathbf{\Theta}_l &= \mathbf{1}\bm{\mu}_l^{\text{T}} + \mathbf{AB}_l^{\text{T}}, \quad l = 1,\ldots,L \\
     \mathbf{1}^{\text{T}}\mathbf{A} &= \mathbf{0}\\
	 \mathbf{A}^{\text{T}}\mathbf{A} &= \mathbf{I}. \\
\end{aligned}
\end{equation}

\subsubsection{Algorithm}
The algorithm developed in Section \ref{section:5.3} can be modified only with respect to the majorization step of the penalty function and the updating $\mathbf{B}_l$ step to fit the P-ESCA model with a composite concave penalty in equation \ref{chapter6_eq:3}. Here we take $\sigma_{lr} = \sum_{j}^{J_l} g_{inner}(|b_{jr}^l|)$ and $\sigma_{ljr} = |b_{jr}^l|$. Since both $g_{out}(\sigma_{lr})$ and $g_{inner}(\sigma_{ljr})$ are concave function and they are monotonically non-decreasing, their composition is also a concave function with respect to $\sigma_{ljr}$. Therefore, we can majorize the composite function $g_{out}(\sum_{j}^{J_l} g_{inner}(\sigma_{ljr}))$ in a similar way  as the equation \ref{chapter5_eq:5}, $g_{out}(\sum_{j}^{J_l} g_{inner}(\sigma_{ljr})) \leq \sum_{j}^{J_l} \omega_{ljr}^k \sigma_{ljr} + c$, in which $\omega_{ljr}^k = \partial g_{out}(\sigma_{lr}^k) \partial g_{inner}(\sigma_{ljr}^k)$ and $\sigma_{ljr}^k$ is the absolute value of the $jr^{\text{th}}$ element of $\mathbf{B}_l^k$ (the $k^{\text{th}}$ approximation of $\mathbf{B}_l$ during the $k^{\text{th}}$ iteration), $\sigma_{lr}^k = \sum_{j}^{J_l} g_{inner}(\sigma_{ljr}^k)$. After majorizing the original problem in equation \ref{chapter6_eq:3}, updating the offset terms $\{\mu_l \}_1^L$ and score matrix $\mathbf{A}$ in exactly the same way as in Section \ref{section:5.3}, the optimization problem associated with the updating of $\mathbf{B}_l$ is
\begin{equation*}
\begin{aligned}
    \min_{\mathbf{B}_l} \quad & \frac{\rho_l}{2\alpha_l} ||\mathbf{AB}_l^{\text{T}} - \mathbf{JH}_{l}^{k}||_F^2  + \lambda_l J_l \sum_{r}^{R} (\sum_{j}^{J_l} \omega_{ljr}^k \sigma_{ljr}) \\
	&= \frac{\rho_l}{2\alpha_l}||\mathbf{B}_l - (\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A}||_F^2 + \lambda_l J_l \sum_{r}^{R} (\sum_{j}^{J_l} \omega_{ljr}^k \sigma_{ljr})\\
    &= \sum_{j}^{J_l} \sum_{r}^R \Big[ \frac{\rho_l}{2\alpha_l}(b_{jr}^l - ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr})^2 + \lambda_l J_l \omega_{ljr}^k \sigma_{ljr} \Big],
\end{aligned}
\end{equation*}
Take $\tilde{\lambda}_{ljr} = \lambda_l J_l \omega_{ljr}^k \alpha_l/\rho_l$ and $v_{ljr} = ((\mathbf{JH}_{l}^{k})^{\text{T}} \mathbf{A})_{jr}$, the analytical solution of $b_{jr}^{l}$ is $b_{jr}^{l} = \text{sign}(v_{ljr}) \max(0, |v_{ljr}| -\tilde{\lambda}_{ljr})$. To update the parameter $\mathbf{B}_l$, we can simply apply this proximal operator to all the elements of $\mathbf{B}_l$. The other parts of the algorithm are the same as in Section \ref{section:5.3}.

\subsection{P-ESCA model with other types of penalties}
All the algorithms for the above P-ESCA models with different penalties are based on the fact that the updating of $\mathbf{B}_l$ in equation \ref{chapter5_eq:7} can be re-expressed as a problem of finding the proximal operator for the $L_2$ norm or the $L_1$ norm penalty. Therefore, P-ESCA model can also be extended to include other types penalty whose proximal operator has simple or analytical solution. For example there is not difficulty in including concave penalties on the rows of the loading matrix $\mathbf{B}_l$ to induce row-wise sparsity, which could be useful for the feature selection. Furthermore, we can also add cardinality constraints (pseudo $L_{0}$ norm) on the number of nonzero elements, the number of nonzero rows, or the number of nonzero columns of the loading matrix $\mathbf{B}_l$ to induce the desired sparsity pattern. These various $L_0$ norm penalties are non-convex, however, there are heuristic solutions for the corresponding proximal operator \cite{boyd2011distributed}. These various $L_0$ penalties can be useful if all our data sets are quantitative. However, when discrete data sets are used, the derived model with the $L_0$ norm type of penalty will have problems in constraining the scale of estimated parameters. The standard logistic PCA model, in which the exact low rank constraint can be regarded as applying $L_0$ norm penalty on the singular values, is a good example to illustrate this point.

\section{Other directions of tackling heterogeneous measurement scales}
In the current thesis, the heterogeneous measurement scales are accounted for by assuming a parametric exponential family distribution in the similar way as the generalized linear models. There also existed other possible directions \cite{kiers1989three, anderson2018xpca, de2009gifi} to tackle the problems induced by the heterogenous measurement scales. One promising alternative is the semi-parametric XPCA method \cite{anderson2018xpca}. In the probabilistic interpretation of a PCA model on a matrix $\mathbf{X}$($I\times J$), we assume we have $I$ samples from a $J$ dimensional multivariate normal distribution and therefore normal marginal distribution for each column. On the contrary, XPCA model is based on a semi-parametric $J$ dimensional multivariate distribution, which is the combination of nonparametric marginals of all the $J$ quantitative or discrete columns and a Gaussian copula. The assumptions of parametric marginal distributions (normal distribution for quantitative data, Bernoulli distribution for binary data) for the columns of the observed data set $\mathbf{X}$ are relaxed in the XPCA model. Therefore, when the exponential family distribution is not a good approximation of the observed data, for example the empirical distribution of a quantitative variable is far from symmetric, XPCA model has a clear advantage. Another interesting alternative is the non-parametric representation matrices approach \cite{kiers1989three}, in which each variable (continuous or discrete) is represented by a representation matrix and the resulting representation matrices can be used in a three way model for symmetric data. The advantage of representative matrices approach is that no probabilistic assumption is made for the model.

\section{Using data fusion for supervised learning}
All the methods developed in this thesis are unsupervised learning approaches. It is worthwhile to extend these methods in the supervised learning framework for prediction tasks. A simple approach, same as the extension of PCA to principal component regression model for prediction tasks, is as follows. These various unsupervised data fusion methods are taken as feature extraction approaches for multiple data sets. The derived low dimensional score matrix can be regarded as the extracted features, and can be used as inputs for any other supervised learning methods. However, the extracted low dimensional features are not necessarily optimal for the prediction tasks. Therefore, when label information is available, it is better to make full use of it to make the extracted features more informative to the prediction tasks. The P-ESCA model can be extended from this perspective in a similar way as extending the PCA model to the partial least squares regression model \cite{geladi1986partial}. The extracted low dimensional structures from the P-ESCA model should not only represent the multiple data sets well, but also have high covariance with the label information.

\section{Incorporating the information of experimental design}
Sometimes the multiple sets of measurements on the same objects result from carefully designed experimental studies rather than observational studies. Such an experimental design always contains several factors, such as different treatments or different time points or their combinations, which are of interest with respect to the research question. Therefore, these experimental factors are underlying the multiple data sets on the same objects. To study the effects of these experimental factors or to remove their effects on the explorative data analysis, the used data fusion approaches should take the experimental design structure into account. The proposed P-ESCA can be extended from this direction by including extra low dimensional structures to account for these experimental factors in a similar way as the ANOVA-simultaneous component analysis (ASCA) model \cite{smilde2005anova}.




























