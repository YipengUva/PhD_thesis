\chapter{Introduction} \label{chapter:1}
In systems biology, it is becoming increasingly common to measure biochemical entities at different levels of the same biological system. Hence, data fusion problems, which focus on analysing such data sets simultaneously to arrive at a holistic understanding of the studied system, are abundant in the life sciences. With the availability of a multitude of measuring techniques one of the central problems is the heterogeneity of the data. In this thesis, we mainly discuss two types of heterogeneity. The first one is the type of \emph{data}, such as metabolomics, proteomics and RNAseq data in genomics. These different omics data reflect the properties of the studied biological system from different perspectives. The second one is the type of \emph{scale}, which indicates the measurements obtained at different scales, such as binary, ordinal, interval and ratio-scaled variables. In genomics, an example is the measurements of gene-expression and point mutation status on the same objects. The latter are binary data and gene-expression measurements are quantitative data. Ideally, data fusion methods should consider these two types of heterogeneity of such measurements and this will be the topic of this thesis.

The goal of this thesis is to develop appropriate statistical methods capable to fuse data sets of the two types heterogeneity. Before going into the details of the developed methods, we begin with a brief introduction of the concept of data fusion in life sciences and the characteristics of the two types of heterogeneity. Another important concept in this thesis is the concave penalty, which is the basis of all the developed methods. Although it is not directly related to the fusion of heterogeneous data sets, it is also introduced in Chapter \ref{chapter:1}. \footnote{This chapter is based on: Smilde, A.K., Song, Y., Westerhuis, J.A., Kiers, H.A., Aben, N. and Wessels, L.F., 2019. ``Heterofusion: Fusing genomics data of different measurement scales.'' arXiv preprint arXiv:1904.10279.}

\section{Data fusion in life sciences}
With the availability of comprehensive measurements collected in multiple related data sets in the life sciences, the need for a simultaneous analysis of such data to arrive at a global view on the system under study is of increasing importance. There are many ways to perform such a simultaneous analysis and these go also under very different names in different areas of data analysis: data fusion, data integration, global analysis, multi-set or multi-block analysis to name a few. We will use the term \emph{data fusion} in this thesis. Data fusion plays an especially important role in the life sciences, e.g., in genomics it is not uncommon to measure gene-expression (array data or RNAseq data), methylation of DNA and copy number variation. Sometimes, also proteomics and metabolomics measurements are available. All these examples serve to show that having methods in place to integrate these data is not a luxury anymore.

Without trying to build a rigorous taxonomy of data fusion it is worthwhile to distinguish several distinctions in data fusion. The first distinction is between model-based and exploratory data fusion. The former uses background knowledge in the form of models to fuse the data; one example being genome-scale models in biotechnology \cite{zimmermann2017integration}. The latter does not rely on models, since these are not available or poorly known, and thus uses empirical modeling to explore the data. In this thesis, we will focus on exploratory data fusion. The next distinction is between low-, medium-, and high-level data fusion \cite{steinmetz1999methodology}. In low-level data fusion, the data sets are combined at the lowest level, that is, at the level of the (preprocessed) measurements. In medium-level data fusion, each separate data set is first summarized, e.g., by using a dimension reduction method or through variable selection. The reduced data sets are subsequently subjected to the data fusion. In high-level data fusion, each data set is used for prediction or classification of an outcome and the prediction or classification results are then combined, e.g, by using majority voting \cite{doeswijk2011increase}. All these types of data fusion have advantages and disadvantages which are beyond the scope of this thesis. In this thesis, we will focus on low- and medium-level fusion.

The final characteristic of data fusion relevant for this thesis is the heterogeneity of the data sets to be fused. Different types of heterogeneity can be distinguished. The first one is the type of \emph{data}, such as metabolomics, proteomics and RNAseq data in genomics. Clearly, these data relate to different parts of the biological system. The second one is the type of \emph{scale} in which the data are measured present in the fusion problem. In genomics, an example is a data set where gene-expressions are available and mutation data in the form of single nucleotide polymorphisms(SNPs). The latter are binary data and gene-expression measurements are quantitative data. They are clearly measured at a different scale. Ideally, data fusion methods should consider these two levels of hterogeneity in data analysis and this will be the topic of this thesis. In the following section we will show the characteristics of these two types of heterogeneity and how they effect the data analysis.

\section{Two types of heterogeneity}

\subsection{Heterogeneous measurement scales in multiple data sets}
Multiple data sets measured on the same objects may have different types of measurement scales. The history of measurement scales goes back a long time. A seminal paper drawing attention to this issue appeared in the 40-ties \cite{Stevens1946}. Since then numerous papers, reports and books have appeared \cite{Suppes1962,Krantz1971,Narens1981,Narens1986,Luce1987,Hand1996}. The measuring process assigns numbers to aspects of objects (an \textit{empirical system}), e.g, weights of persons. Hence, measurements can be regarded as a mapping from the empirical system to numbers, and scales are properties of these mappings. In measurement theory, there are two fundamental theorems \cite{Krantz1971}: the representation theorem and the uniqueness theorem. The \textit{representation theorem} asserts the axioms to be imposed on an empirical system to allow for a homomorphism of that system to a set of numerical values. Such a homomorphism into the set of real numbers is called a scale and thus represents the empirical system. A scale possesses \textit{uniqueness} properties: we can measure the weight of persons in kilograms or in grams, but if one person weighs twice as much as another person, this ratio holds true regardless the measurement unit. Hence, weight is a so-called ratio-scaled variable and this ratio is unique. The transformation of measuring in grams instead of kilograms is called a \textit{permissible} transformation since it does not change the ratio of two weights. For a ratio-scaled variable, only similarity transformations are permissible; i.e. $\widetilde{x}=\alpha x; \alpha>0$ where $x$ is the variable on the original scale and $\widetilde{x}$ is the variable on the transformed scale. This is because
\begin{equation*}\label{eRatio}
 \frac{\widetilde{x_i}}{\widetilde{x_j}}=\frac{\alpha x_i}{\alpha x_j}=\frac{x_i}{x_j}.
\end{equation*}
Note that this coincides with the intuition that the unit of measurement is immaterial.

The next level of scale is the interval-scaled measurement. The typical example of such a scale is concentrations of metabolites in metabolomics research and the permissible transformation is affine, i.e. $\widetilde{x}=\alpha x +\beta; \alpha>0$. In that case, the ratio of two intervals is unique because
\begin{equation*}\label{eInterval}
 \frac{\widetilde{x_i}-\widetilde{x_j}}{\widetilde{x_k}-\widetilde{x_l}}=\frac{(\alpha x_i + \beta)-(\alpha x_j + \beta)}{(\alpha x_k + \beta)-(\alpha x_l + \beta)}=\frac{\alpha (x_i-x_j)}{\alpha (x_k-x_l)}=\frac{x_i-x_j}{x_k-x_l}.
\end{equation*}
Stated differently, the zero point and the unit are arbitrary on this scale.

Ordinal-scaled variables represent the next level of measurements. Typical examples are scales of agreement in surveys: strongly disagree, disagree, neutral, agree and strongly agree. There is a rank-order in these answers, but no relationship in terms of ratios or intervals. The permissible transformation of an ordinal-scale is a monotonic increasing transformation since such transformations keep the order of the original scale intact. Nominal-scaled variables are next on the list. These variables are used to encode categories and are sometimes also called categorical. Typical example are gender, race, brands of cars and the like. The only permissible transformation for a nominal-scaled variable is the one-to-one mapping. A special case of a nominal-scaled variable is the binary (0/1) scale. Binary data can have different meanings; they can be used as categories (e.g. gender) and are then nominal-scale variables. They can also be two points on a higher-level scale, such as absence and presence (e.g. for methylation data).

The above four scales are the most used ones but others exists \cite{Suppes1962,Krantz1971}. Counts, e.g., have a fixed unit and are therefore sometimes called absolute-scaled variables \cite{Narens1986}. Another scale is the one for which the power transformation is permissible; i.e. $\widetilde{x}=\alpha x^\beta; \alpha, \beta>0$ which is called a log-interval scale because a logarithmic transformation of such a scale results in an interval-scale. An example is density \cite{Krantz1971}. Sometimes the scales are lumped in quantitative (i.e. ratio and interval) and qualitative (ordinal and nominal) data.

An interesting aspect of measurement scales is to what extent meaningful statistics can be derived from such scales (see Table 1 in \cite{Stevens1946}). A prototypical example is using a mean of a sample of nominal-scaled variables which is generally regarded as being meaningless. This has also provoked a lot of discussion \cite{Adams1965,Hand1996} and there are nice counter-examples of apparently meaningless statistics that still convey information about the empirical system \cite{Michell1986}. As always, the world is not black or white.

In practice we also use other taxonomies to classify the types of measurements \cite{agresti2013categorical}. A commonly used one is the Discrete-Continuous variable distinction according to whether or not the possible number of values is countable. Therefore, binary, nominal and ordinal scaled measurements are discrete while ratio and interval scaled measurements are continuous. Another commonly used taxonomy is the Quantitative-Qualitative variable distinction, which depends on whether two different measurements differ in quality or in quantity. Thus nominal scaled measurements are qualitative while ratio and interval scaled measurements are quantitative. And the ordinal scaled measurements have the characteristics of both quantitative and quantitative variables.

\subsection{Heterogeneous information in multiple data sets}
Multiple sets of measurements on the same objects obtained from different platforms may reflect partially complementary information of the studied system. Therefore, these multiple data sets may contain heterogenous information, the information that is common across all or some of the data sets, and the information which is specific to each data set (often called distinct). The challenge for the data fusion of such data sets is how to separate the common and distinct information existed in multiple data sets. These different sources of information have to be disentangled from every data set to have a holistic understanding of the studied system. Here we focus on using common and distinct components to approximate the common and distinct variation (information) existing in multiple data sets measured on the same objects \cite{smilde2017common}. We will use a simultaneous component analysis (SCA) model with structural sparsity patterns on the loading matrix \cite{gaynanova2017structural} as an example to show the idea.

A classical SCA model tries to discover the common subspace between multiple data sets to represent the common information between these data sets. Suppose the quantitative measurements from $L$ different platforms on the same $I$ objects result into $L$ quantitative data sets, $\left\{\mathbf{X}_l \right\}_{l=1}^{L}$, and the $l^{\text{th}}$ data set $\mathbf{X}_l$($I \times J_l$) has $J_l$ variables. After these data sets are column centered, we can decompose them in the SCA model framework as $\mathbf{X}_l = \mathbf{AB}_l^{\text{T}} + \mathbf{E}_l$, in which $\mathbf{A}$($I\times R$) is the common score matrix; $\mathbf{B}_l$($J_l\times R$) and $\mathbf{E}_l$($I\times J_l$) are the loading matrix and residual term respectively for $\mathbf{X}_l$ and $R$ is the number of components. The common column subspace, which is spanned by the columns of the score matrix $\mathbf{A}$, represents the common information between these $L$ data sets.

The drawback of the SCA model is that only the global common components, which account for the common variation across all the data sets, is modeled. However, the real situation in multiple data sets integration can be far more complex as local common variation across some of the data sets and distinct variation in each data set are expected as well. With the help of the concept of structural sparsity on the loading matrices of a SCA model, we can interpret the common and distinct variation framework as follows. Suppose we construct a SCA model on three column centered quantitative data sets $\left\{\mathbf{X}_l \right\}_{l=1}^{3}$, the common score matrix is $\mathbf{A}$, the corresponding loading matrices are $\left\{\mathbf{B}_l \right\}_{l=1}^{3}$, and $\mathbf{X}_l = \mathbf{A}\mathbf{B}_{l}^{\text{T}} + \mathbf{E}_l$, in which $\mathbf{E}_l$ is the residual term for $l^{\text{th}}$ data set. If the structured sparsity pattern in $\left\{\mathbf{B}_l \right\}_{l=1}^{3}$ is expressed as follows,
\begin{equation*}
\begin{aligned}
   \left(
                 \begin{array}{c}
                   \mathbf{B}_1 \\
                   \mathbf{B}_2 \\
                   \mathbf{B}_3 \\
                 \end{array}
               \right)
               = \left(
                   \begin{array}{cccccccc}
                     \mathbf{b}_{1,1} & \mathbf{b}_{1,2} & \mathbf{b}_{1,3} & \mathbf{0}       & \mathbf{b}_{1,5} & \mathbf{0}       & \mathbf{0}      \\
                     \mathbf{b}_{2,1} & \mathbf{b}_{2,2} & \mathbf{0}       & \mathbf{b}_{2,4} & \mathbf{0}       & \mathbf{b}_{2,6} & \mathbf{0}       \\
                     \mathbf{b}_{3,1} & \mathbf{0}       & \mathbf{b}_{3,3} & \mathbf{b}_{3,4} & \mathbf{0}       & \mathbf{0}       & \mathbf{b}_{3,7} \\
                   \end{array}
                 \right),
\end{aligned}
\end{equation*}
in which $\mathbf{b}_{l,r} \in \mathbf{R}^{J_l}$ indicates the $r^{\text{th}}$ column of the $l^{\text{th}}$ loading matrix $\mathbf{B}_l$, then we have the following relationships,
\begin{equation*}
\begin{aligned}
   \mathbf{X}_1 & = \mathbf{a}_1\mathbf{b}_{1,1}^{\text{T}} &+& \mathbf{a}_2\mathbf{b}_{1,2}^{\text{T}} &+& \mathbf{a}_3\mathbf{b}_{1,3}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{a}_5\mathbf{b}_{1,5}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{0}  &+& \mathbf{E}_1                   \\
   \mathbf{X}_2 & = \mathbf{a}_1\mathbf{b}_{2,1}^{\text{T}} &+& \mathbf{a}_2\mathbf{b}_{2,2}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{a}_4\mathbf{b}_{2,4}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{a}_6\mathbf{b}_{2,6}^{\text{T}} &+& \mathbf{0}  &+& \mathbf{E}_2                   \\
   \mathbf{X}_3 & = \mathbf{a}_1\mathbf{b}_{3,1}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{a}_3\mathbf{b}_{3,3}^{\text{T}} &+& \mathbf{a}_4\mathbf{b}_{3,4}^{\text{T}} &+& \mathbf{0}                     &+& \mathbf{0}                     &+& \mathbf{a}_7\mathbf{b}_{3,7}^{\text{T}} &+& \mathbf{E}_3 .
\end{aligned}
\end{equation*}
Here $\mathbf{a}_r$ indicates the $r^{\text{th}}$ column of the common score matrix $\mathbf{A}$. The first component represents the global common variation across three data sets; the $2^{\text{nd}}$, $3^{\text{nd}}$ and $4^{\text{nd}}$ components represent the local common variation across two data sets and the $5^{\text{nd}}$, $6^{\text{nd}}$ and $7^{\text{nd}}$ components represent the distinct variation specific to each single data set. Therefore, the heterogeneity of data (common and distinct information existed in multiple data sets) is disentangled by the common and distinct components.

\section{Using concave penalties to induce sparsity}
Although concave penalties are not directly related to the fusion of heterogeneous data sets, they are the basis of all the developed methods in this thesis. Therefore, it is better to have a general introduction of them at the beginning.

The comprehensive measurements in the current biological research always result into high dimensional data sets, of which the number of variables are much larger than the number of samples. For the analysis of such high dimensional data sets, sparse parameter estimation (many estimated parameters are exactly 0) is always desired since it makes both the data analysis problem feasible and the results easier to be interpreted. Some typical examples of sparse parameter estimation include the sparse regression models \cite{fan2001variable}, the low rank matrix approximation problems \cite{jolliffe2002principal, smilde2017common, gavish2017optimal}, the structure learning problems in graphical models \cite{friedman2008sparse}, and many others \cite{tibshirani2005sparsity, witten2009penalized,huang2012selective}.

We can use a linear regression model as an example to illustrate how to achieve sparse parameter estimation through various penalties. Suppose we have a univariate response variable $y\in \mathbb{R}$ and a multivariate explanatory variable $\mathbf{x} \in \mathbb{R}^{J}$. A standard linear regression model can be expressed as $y = \mathbf{x}^{\text{T}}\bm{\beta} + \epsilon$, in which $\bm{\beta} \in \mathbb{R}^{J}$ is the coefficient vector and $\epsilon \in \mathbb{R}$ is the error term following a normal distribution with mean $0$ and variance $\sigma^2$, $\epsilon \sim N(0,\sigma^2)$. After obtaining $I$ samples of $\{y, \mathbf{x}\}$, we have the data sets $\{y_i, \mathbf{x}_i\}_{i=1}^{I}$, which can be further expressed in their vector and matrix form as $\mathbf{y} \in \mathbb{R}^{I}$ and $\mathbf{X} \in \mathbb{R}^{I\times J}$. The optimization problem associated with the standard linear model is $\min_{\bm{\beta}} ~ \frac{1}{2}||\mathbf{y} - \mathbf{X}\bm{\beta}||^2$, and the analytical form solution is $\hat{\bm{\beta}} = (\mathbf{X}^{\text{T}}\mathbf{X})^{-1}\mathbf{X}^{\text{T}}\mathbf{y}$. Unfortunately, this model is unidentifiable when $J>I$ and ill-conditioned when the explanatory variables are correlated. Also the estimated coefficient vector $\hat{\bm{\beta}}$ is always dense, which makes the interpretation difficult.

Cardinality constraint can be imposed on the linear regression model as a hard constraint to induce a sparse parameter estimation of $\bm{\beta}$. If we require only $R$ elements of $\bm{\beta}$ to be nonzero, the associated optimization problem can be expressed as $\min_{\bm{\beta}} ~ \frac{1}{2}||\mathbf{y} - \mathbf{X}\bm{\beta}||^2 ~ \text{subject to} ~ ||\bm{\beta}||_0 = R$, in which $||\bm{\beta}||_0 = R$ is the cardinality constraint, $||~||_0$ indicates the pseudo $L_{0}$ norm and counts the number of nonzero elements. Since the above optimization problem is non-convex and difficult to solve, the cardinality constraint is always replaced by its convex relaxation $L_1$ norm to induce the sparsity, which results in the lasso regression model \cite{tibshirani1996regression}. The optimization problem associated with the lasso regression model is $\min_{\bm{\beta}} ~ \frac{1}{2}||\mathbf{y} - \mathbf{X}\bm{\beta}||^2 + \lambda ||\bm{\beta}||_1$ in which $\lambda$ is the tuning parameter and $||~||_1$ indicates the $L_{1}$ norm. Efficient algorithms exist to solve this convex optimization problem \cite{efron2004least,parikh2014proximal}. However, the $L_{1}$ norm penalty shrinks all the elements of the coefficient vector $\bm{\beta}$ to the same degree. This leads to a biased estimation of the coefficients with large absolute values. This behavior will further make the prediction error or CV error based model selection procedure inconsistent \cite{meinshausen2010stability}. Many non-convex penalties, most of them are concave functions with respect to the absolute value of $\bm{\beta}$, have been proposed to tackle the drawback of the $L_{1}$ norm penalty \cite{fan2001variable,armagan2013generalized}. They can shrink the parameters in a nonlinear manner to achieve both nearly unbiased and sparse parameter estimation. Take the frequentist version of the generalized double Pareto (GDP) \cite{armagan2013generalized} shrinkage as an example. The optimization problem of a linear regression model with the GDP penalty can be expressed as $\min_{\bm{\beta}} ~ \frac{1}{2}||\mathbf{y} - \mathbf{X}\bm{\beta}||^2 + \lambda \sum_{j}^{J} \log(1+\frac{|\beta_j|}{\gamma})$, in which $\beta_j$ is the $j^{\text{th}}$ element of $\bm{\beta}$, $\gamma$ is a hyper-parameter, and $\log(1+\frac{|\beta_j|}{\gamma})$ is the concave GDP penalty on $\beta_j$. The thresholding properties of the cardinality constraint, the $L_{1}$ norm penalty and the GDP penalty with different values of $\gamma$ are shown in Fig.~\ref{chapter1_fig:1}. The cardinality constraint shrinks all the coefficients, whose absolute values are less than a selected threshold, to 0 while keep other coefficients unchanged. Usually, this thresholding behavior is called hard thresholding \cite{tibshirani1996regression}. The $L_{1}$ norm penalty shrinks all the coefficients to the same degree until some coefficients are 0. Usually, this thresholding behavior is called soft thresholding \cite{tibshirani1996regression}. On the other hand, GDP penalty shrinks the coefficients in a nonlinear manner according to the absolute values of the corresponding coefficients. In this way, coefficients with small absolute values are more likely to be shrunk to 0, while coefficients with large absolute values tend to be shrunk less.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{Chapter1_Fig_1.pdf}
    \caption{Thresholding properties of the cardinality constraint, $L_{1}$ norm, GDP penalties when the same degree of shrinkage is achieved. Legend cardinality: cardinality constraint, legend $L_{1}$: $L_{1}$ norm penalty, legend original: the original values before thresholding. $\beta$ in $x$ axis indicates the original value of the coefficient while $\eta$ in $y$ axis is the value after thresholding.}
    \label{chapter1_fig:1}
\end{figure}

\section{Scope and outline of the thesis}
Principal component analysis (PCA) model is the basis of many commonly used data fusion methods \cite{maage2019performance}. And both PCA and these data fusion methods assume the used data sets are quantitative. Thus, before talking about the data fusion of data sets with heterogeneous measurement scales, we should first introduce the generalizations of PCA for the qualitative data set. We review the extensions of PCA methods for the analysis of multivariate binary data sets in Chapter \ref{chapter:2} and develop a robust logistic PCA method in Chapter \ref{chapter:3}. After that, we are ready for the data fusion of data sets with heterogeneous measurement scales. We generalize the commonly used data fusion method, simultaneous component analysis (SCA), in a probabilistic framework for the data fusion of the multivariate quantitative and binary measurements data sets in Chapter \ref{chapter:4}. Finally, it comes to the data fusion of data sets of the two types heterogeneity. We develop an exponential family SCA model for the data fusion of multiple data sets of mixed data types, such as quantitative, binary or count, and introduced the nearly unbiased group concave penalty to induce structured sparsity on the loading matrix to separate common (global and local) and distinct variation in such mixed types data sets in Chapter \ref{chapter:5}. Then the thesis closes in Chapter \ref{chapter:6} with an outlook into the future of fusing heterogeneous data sets. 